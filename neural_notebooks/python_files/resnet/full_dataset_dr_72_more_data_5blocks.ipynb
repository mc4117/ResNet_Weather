{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train a CNN\n",
    "\n",
    "In this notebook we will go through all the steps required to train a fully convolutional neural network. Because this takes a while and uses a lot of GPU RAM a separate command line script (`train_nn.py`) is also provided in the `src` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Depending on your combination of package versions, this can raise a lot of TF warnings... \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from src.score import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def limit_mem():\n",
    "    \"\"\"By default TF uses all available GPU memory. This function prevents this.\"\"\"\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "DATADIR = '/rds/general/user/mc4117/home/WeatherBench/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create data generator\n",
    "\n",
    "First up, we want to write our own Keras data generator. The key advantage to just feeding in numpy arrays is that we don't have to load the data twice because our intputs and outputs are the same data just offset by the lead time. Since the dataset is quite large and we might run out of CPU RAM this is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the validation subset of the data: 2017 and 2018\n",
    "z500_valid = load_test_data(f'{DATADIR}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{DATADIR}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "var_dict = {\n",
    "    'geopotential': ('z', [500, 850]),\n",
    "    'temperature': ('t', [500, 850]),\n",
    "    'specific_humidity': ('q', [850]),\n",
    "    '2m_temperature': ('t2m', None),\n",
    "    'potential_vorticity': ('pv', [50, 100]),\n",
    "    'constants': ['lsm', 'orography']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "ds = [xr.open_mfdataset(f'{DATADIR}/{var}/*.nc', combine='by_coords') for var in var_dict.keys()]\n",
    "ds_whole = xr.merge(ds, compat = 'override')\n",
    "\n",
    "# load all training data\n",
    "ds_train = ds_whole.sel(time=slice('1979', '2016'))\n",
    "ds_test = ds_whole.sel(time=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True,\n",
    "                 mean=None, std=None, output_vars=None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        level_names = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for long_var, params in var_dict.items():\n",
    "            if long_var == 'constants':\n",
    "                for var in params:\n",
    "                    data.append(ds[var].expand_dims(\n",
    "                        {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "                    ))\n",
    "                    level_names.append(var)\n",
    "            else:\n",
    "                var, levels = params\n",
    "                try:\n",
    "                    data.append(ds[var].sel(level=levels))\n",
    "                    level_names += [f'{var}_{level}' for level in levels]\n",
    "                except ValueError:\n",
    "                    data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "                    level_names.append(var)\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.data['level_names'] = xr.DataArray(\n",
    "            level_names, dims=['level'], coords={'level': self.data.level})\n",
    "        if output_vars is None:\n",
    "            self.output_idxs = range(len(dg_valid.data.level))\n",
    "        else:\n",
    "            self.output_idxs = [i for i, l in enumerate(self.data.level_names.values)\n",
    "                                if any([bool(re.match(o, l)) for o in output_vars])]\n",
    "\n",
    "        # Normalize\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "#         self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        self.std = self.data.std(('time', 'lat', 'lon')).compute() if std is None else std\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time, level=self.output_idxs).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "bs=32\n",
    "lead_time=72\n",
    "output_vars = ['z_500', 't_850']\n",
    "\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(\n",
    "    ds_train.sel(time=slice('1979', '2015')), var_dict, lead_time, batch_size=bs, load=False, output_vars = output_vars)\n",
    "#dg_valid = DataGenerator(\n",
    "#    ds_train.sel(time=slice('2016', '2016')), var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False, output_vars = output_vars)\n",
    "\n",
    "# Now also a generator for testing. Impartant: Shuffle must be False!\n",
    "dg_test = DataGenerator(ds_test, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std,\n",
    "                         shuffle=False, output_vars=output_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only batch normalisation\n",
    "# produced by resnet/whole_mm_more_data.py with 5 blocks\n",
    "\n",
    "pred_ensemble_1 = np.load('/rds/general/user/mc4117/ephemeral/saved_pred/whole_res_more_data_do_5_0.npy')\n",
    "pred_ensemble_2 = np.load('/rds/general/user/mc4117/ephemeral/saved_pred/whole_res_more_data_do_5_1.npy')\n",
    "#pred_ensemble_3 = np.load('/rds/general/user/mc4117/ephemeral/saved_pred/whole_res_more_data_2.npy')\n",
    "#pred_ensemble_4 = np.load('/rds/general/user/mc4117/ephemeral/saved_pred/whole_res_more_data_3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "conflicting sizes for dimension 'ens': length 12 on the data but length 1 on coordinate 'ens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-ccba31da419c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mpred_ensemble_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lat'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lon'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n\u001b[0m\u001b[1;32m      7\u001b[0m                 },\n\u001b[1;32m      8\u001b[0m     ),\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, coords, dims, name, attrs, indexes, fastpath)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_data_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_compatible_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_coords_and_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m             \u001b[0mvariable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             indexes = dict(\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m_infer_coords_and_dims\u001b[0;34m(shape, coords, dims)\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0;34m\"conflicting sizes for dimension %r: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0;34m\"length %s on the data but length %s on \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                     \u001b[0;34m\"coordinate %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 )\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: conflicting sizes for dimension 'ens': length 12 on the data but length 1 on coordinate 'ens'"
     ]
    }
   ],
   "source": [
    "samples = 1\n",
    "preds_1 = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        pred_ensemble_1[0, ...],\n",
    "        dims=['time', 'lat', 'lon', 'ens'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        pred_ensemble_1[1, ...],\n",
    "        dims=['time', 'lat', 'lon', 'ens'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "preds_2 = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        pred_ensemble_2[0, ...],\n",
    "        dims=['time', 'lat', 'lon', 'ens'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        pred_ensemble_2[1, ...],\n",
    "        dims=['time', 'lat', 'lon', 'ens'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "\"\"\"\n",
    "preds_3 = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        pred_ensemble_3[0, ...],\n",
    "        dims=['time', 'lat', 'lon', 'ens'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        pred_ensemble_3[1, ...],\n",
    "        dims=['time', 'lat', 'lon', 'ens'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "preds_4 = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        pred_ensemble_4[0, ...],\n",
    "        dims=['time', 'lat', 'lon', 'ens'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        pred_ensemble_4[1, ...],\n",
    "        dims=['time', 'lat', 'lon', 'ens'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, 'ens': np.arange(samples), \n",
    "                },\n",
    "    )\n",
    "})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = dg_test[0]\n",
    "\n",
    "for i in range(1, len(dg_test)):\n",
    "    X2, y2 = dg_test[i]\n",
    "    y1 = np.concatenate((y1, y2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_unnorm =y1* dg_test.std.isel(level=[0,3]).values+dg_test.mean.isel(level=[0,3]).values\n",
    "\n",
    "real_ds = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        real_unnorm[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        real_unnorm[..., 1],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rmse = compute_weighted_rmse(preds_1, real_ds).compute()\n",
    "print(cnn_rmse)\n",
    "\n",
    "cnn_rmse_2 = compute_weighted_rmse(preds_2, real_ds).compute()\n",
    "print(cnn_rmse_2)\n",
    "\n",
    "#cnn_rmse_3 = compute_weighted_rmse(preds_3, real_ds).compute()\n",
    "#print(cnn_rmse_3)\n",
    "\n",
    "#cnn_rmse_4 = compute_weighted_rmse(preds_4, real_ds).compute()\n",
    "#print(cnn_rmse_4)\n",
    "\n",
    "preds_total = (preds_1 + preds_2)/2 # + preds_3 + preds_4)/4\n",
    "\n",
    "cnn_rmse_total = compute_weighted_rmse(preds_total, real_ds).compute()\n",
    "cnn_rmse_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_total_2 = compute_weighted_rmse((preds_1+preds_2)/2, real_ds)\n",
    "cnn_total_3 = compute_weighted_rmse((preds_1+preds_2+preds_3)/3, real_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# effect of doing ensembles\n",
    "plt.scatter(cnn_rmse.z, cnn_rmse.t)\n",
    "plt.scatter(cnn_rmse_2.z, cnn_rmse_2.t)\n",
    "plt.scatter(cnn_rmse_3.z, cnn_rmse_3.t)\n",
    "plt.scatter(cnn_rmse_4.z, cnn_rmse_4.t)\n",
    "plt.plot([cnn_rmse.z, cnn_total_2.z, cnn_total_3.z, cnn_rmse_total.z], [cnn_rmse.t, cnn_total_2.t, cnn_total_3.t, cnn_rmse_total.t], 'o-', label = 'Ensemble mean')\n",
    "\n",
    "extra_z = [np.array(615.81100858), np.array(606.20085112), np.array(603.49767756), np.array(602.63237168)]\n",
    "extra_t = [np.array(2.75629372), np.array(2.7187251), np.array(2.70695138), np.array(2.70178747)]\n",
    "plt.plot(extra_z, extra_t, label = 'extra data ensemble')\n",
    "plt.scatter([623.6], [2.856], label = 'Original model')\n",
    "plt.xlabel('RMSE_z500')\n",
    "plt.ylabel('RMSE_t850')\n",
    "plt.legend()\n",
    "plt.savefig('full_moredata.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ens_t = sum([real_ds.t.isel(time = i)-preds_total.t.isel(time = i) for i in range(len(real_ds.time))])/len(real_ds.time)\n",
    "mean_ens_t.plot()\n",
    "plt.show()\n",
    "mean_ens_z = sum([real_ds.z.isel(time = i)-preds_total.z.isel(time = i) for i in range(len(real_ds.time))])/len(real_ds.time)\n",
    "mean_ens_z.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meansq_ens_z = sum([(real_ds.z.isel(time = i)-preds_total.z.isel(time = i))**2 for i in range(len(real_ds.time))])/len(real_ds.time)\n",
    "(meansq_ens_z-mean_ens_z**2).plot()\n",
    "plt.title('Standard deviation z500')\n",
    "plt.show()\n",
    "meansq_ens_t = sum([(real_ds.t.isel(time = i)-preds_total.t.isel(time = i))**2 for i in range(len(real_ds.time))])/len(real_ds.time)\n",
    "(meansq_ens_t-mean_ens_t**2).plot()\n",
    "plt.title('Standard deviation T850')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6 (test1)",
   "language": "python",
   "name": "python3_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
