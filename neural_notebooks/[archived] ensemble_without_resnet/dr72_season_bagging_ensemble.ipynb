{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Depending on your combination of package versions, this can raise a lot of TF warnings... \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from src.score import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def limit_mem():\n",
    "    \"\"\"By default TF uses all available GPU memory. This function prevents this.\"\"\"\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(0)\n",
    "limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "DATADIR = '/rds/general/user/mc4117/home/WeatherBench/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create data generator\n",
    "\n",
    "First up, we want to write our own Keras data generator. The key advantage to just feeding in numpy arrays is that we don't have to load the data twice because our intputs and outputs are the same data just offset by the lead time. Since the dataset is quite large and we might run out of CPU RAM this is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the validation subset of the data: 2017 and 2018\n",
    "z500_valid = load_test_data(f'{DATADIR}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{DATADIR}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = xr.open_mfdataset(f'{DATADIR}geopotential_500/*.nc', combine='by_coords')\n",
    "t = xr.open_mfdataset(f'{DATADIR}temperature_850/*.nc', combine='by_coords').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "datasets = [z, t]\n",
    "ds = xr.merge(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook let's only load a subset of the training data\n",
    "\n",
    "ds_2015_06 = ds.sel(time = '2015-06')\n",
    "ds_2015_07 = ds.sel(time = '2015-07')\n",
    "ds_2015_08 = ds.sel(time = '2015-08')\n",
    "ds_2016_06 = ds.sel(time = '2016-06')\n",
    "ds_2016_07 = ds.sel(time = '2016-07')\n",
    "ds_2016_08 = ds.sel(time = '2016-08')\n",
    "ds_2017_06 = ds.sel(time = '2017-06')\n",
    "ds_2017_07 = ds.sel(time = '2017-07')\n",
    "ds_2017_08 = ds.sel(time = '2017-08')\n",
    "ds_2018_06 = ds.sel(time = '2018-06')\n",
    "ds_2018_07 = ds.sel(time = '2018-07')\n",
    "ds_2018_08 = ds.sel(time = '2018-08')\n",
    "\n",
    "\n",
    "ds_train = xr.merge([ds_2015_06, ds_2015_07, ds_2015_08, ds_2016_06, ds_2016_07, ds_2016_08])\n",
    "ds_test = xr.merge([ds_2017_06, ds_2017_07, ds_2017_08, ds_2018_06, ds_2018_07, ds_2018_08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4416"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_train.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, mean=None, std=None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for var, levels in var_dict.items():\n",
    "            try:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "            except ValueError:\n",
    "                data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "        self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({'z': None, 't': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32\n",
    "lead_time=72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(ds_train, dic, lead_time, batch_size=bs, load=True)\n",
    "#dg_valid = DataGenerator(ds_valid, dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "# Now also a generator for testing. Impartant: Shuffle must be False!\n",
    "dg_test = DataGenerator(ds_test, dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = dg_train[0]\n",
    "\n",
    "for i in range(1, len(dg_train)):\n",
    "    X2, y2 = dg_train[i]\n",
    "    X1 = np.concatenate((X1, X2))\n",
    "    y1 = np.concatenate((y1, y2))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create and train model\n",
    "\n",
    "Next up, we need to create the model architecture. Here we will use a fully connected convolutional network. Because the Earth is periodic in longitude, we want to use a periodic convolution in the lon-direction. This is not implemented in Keras, so we have to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'pad_width': self.pad_width})\n",
    "        return config\n",
    "\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 conv_kwargs={},\n",
    "                 **kwargs, ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        if type(kernel_size) is not int:\n",
    "            assert kernel_size[0] == kernel_size[1], 'PeriodicConv2D only works for square kernels'\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        self.conv = Conv2D(\n",
    "            filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'filters': self.filters, 'kernel_size': self.kernel_size, 'conv_kwargs': self.conv_kwargs})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def build_cnn(filters, kernels, input_shape, dr=0, fine_tune = False):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)   \n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        if k == 5:\n",
    "            x = PeriodicConv2D(f, k)(x)  \n",
    "        else:\n",
    "            x = PeriodicConv2D(f, k)(x)\n",
    "            x = PeriodicConv2D(f, k)(x)     \n",
    "        x = LeakyReLU()(x)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)     \n",
    "    return keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create predictions\n",
    "\n",
    "Now that we have our model we need to create a prediction NetCDF file. This function does this. \n",
    "\n",
    "We can either directly predict the target lead time (e.g. 5 days) or create an iterative forecast by chaining together many e.g. 6h forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create predictions for non-iterative model\"\"\"\n",
    "    preds = model.predict(dg)\n",
    "    # Unnormalize\n",
    "    preds = preds * dg.std.values + dg.mean.values\n",
    "    fcs = []\n",
    "    lev_idx = 0\n",
    "    for var, levels in OrderedDict({'z': None, 't': None}).items():\n",
    "        if levels is None:\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx],\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx:lev_idx+nlevs],\n",
    "                dims=['time', 'lat', 'lon', 'level'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon, 'level': levels},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(fcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500_valid = load_test_data(f'{DATADIR}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{DATADIR}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 17s 51ms/sample - loss: 9.9011 - val_loss: 4.4558\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 3.1380 - val_loss: 2.5673\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 2.1320 - val_loss: 1.7927\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.7194 - val_loss: 1.6171\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.6018 - val_loss: 1.5479\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.5347 - val_loss: 1.4709\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.4798 - val_loss: 1.4315\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.4369 - val_loss: 1.3897\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3994 - val_loss: 1.3564\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.3650 - val_loss: 1.3244\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3366 - val_loss: 1.3001\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.3115 - val_loss: 1.2755\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2880 - val_loss: 1.2609\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2750 - val_loss: 1.2445\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.2580 - val_loss: 1.2258\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2429 - val_loss: 1.2153\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.2344 - val_loss: 1.2085\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2238 - val_loss: 1.1979\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2146 - val_loss: 1.1993\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2155 - val_loss: 1.1965\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 16s 49ms/sample - loss: 9.8900 - val_loss: 4.4919\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 3.1205 - val_loss: 2.6419\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 2.1438 - val_loss: 1.8350\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.8061 - val_loss: 1.6968\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.6770 - val_loss: 1.6555\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.6038 - val_loss: 1.5843\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.5379 - val_loss: 1.5295\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.4907 - val_loss: 1.4822\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.4433 - val_loss: 1.4404\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.4015 - val_loss: 1.4035\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3640 - val_loss: 1.3720\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3316 - val_loss: 1.3441\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3069 - val_loss: 1.3243\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2910 - val_loss: 1.3143\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2719 - val_loss: 1.2965\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2548 - val_loss: 1.2824\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.2406 - val_loss: 1.2704\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.2278 - val_loss: 1.2609\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2183 - val_loss: 1.2531\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2104 - val_loss: 1.2486\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 16s 49ms/sample - loss: 9.1831 - val_loss: 4.5395\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 3.0341 - val_loss: 2.4162\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 2.0806 - val_loss: 1.7444\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.7765 - val_loss: 1.6563\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.6558 - val_loss: 1.5738\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.5763 - val_loss: 1.4933\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.5101 - val_loss: 1.4365\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.4541 - val_loss: 1.3816\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.4056 - val_loss: 1.3402\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.3634 - val_loss: 1.2990\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.3319 - val_loss: 1.2718\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3043 - val_loss: 1.2551\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2860 - val_loss: 1.2280\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2689 - val_loss: 1.2183\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2593 - val_loss: 1.2060\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2439 - val_loss: 1.1911\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2343 - val_loss: 1.1785\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.2262 - val_loss: 1.1791\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2189 - val_loss: 1.1720\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.2115 - val_loss: 1.1607\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 17s 51ms/sample - loss: 10.9842 - val_loss: 4.2606\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 3.2124 - val_loss: 2.7153\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 2.2166 - val_loss: 1.9159\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.8345 - val_loss: 1.7780\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.7280 - val_loss: 1.7196\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.6639 - val_loss: 1.6610\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.6115 - val_loss: 1.6205\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.5679 - val_loss: 1.5774\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.5282 - val_loss: 1.5372\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.4907 - val_loss: 1.5040\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.4505 - val_loss: 1.4693\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.4151 - val_loss: 1.4342\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3834 - val_loss: 1.4013\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3563 - val_loss: 1.3762\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3347 - val_loss: 1.3548\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3142 - val_loss: 1.3340\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3007 - val_loss: 1.3217\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2848 - val_loss: 1.3051\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2744 - val_loss: 1.3075\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.2622 - val_loss: 1.2804\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 17s 50ms/sample - loss: 10.1988 - val_loss: 4.2537\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 3.1100 - val_loss: 2.4277\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 2.1164 - val_loss: 1.9218\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.8077 - val_loss: 1.7615\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.6964 - val_loss: 1.6757\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.6297 - val_loss: 1.6088\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.5769 - val_loss: 1.5634\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.5313 - val_loss: 1.5163\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.4853 - val_loss: 1.4748\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.4461 - val_loss: 1.4315\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.4079 - val_loss: 1.3968\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3776 - val_loss: 1.3681\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3519 - val_loss: 1.3424\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3320 - val_loss: 1.3267\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3144 - val_loss: 1.3066\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2988 - val_loss: 1.2900\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2853 - val_loss: 1.2762\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2730 - val_loss: 1.2636\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2621 - val_loss: 1.2548\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2538 - val_loss: 1.2471\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 17s 50ms/sample - loss: 10.4993 - val_loss: 3.8932\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 3.0201 - val_loss: 2.3221\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 2.0191 - val_loss: 1.7819\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.7204 - val_loss: 1.5966\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 13s 41ms/sample - loss: 1.6096 - val_loss: 1.5049\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 13s 39ms/sample - loss: 1.5401 - val_loss: 1.4484\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.4900 - val_loss: 1.4057\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.4503 - val_loss: 1.3708\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 13s 39ms/sample - loss: 1.4136 - val_loss: 1.3348\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 13s 38ms/sample - loss: 1.3785 - val_loss: 1.3013\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 12s 38ms/sample - loss: 1.3460 - val_loss: 1.2749\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 13s 38ms/sample - loss: 1.3202 - val_loss: 1.2538\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.2999 - val_loss: 1.2383\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.2836 - val_loss: 1.2224\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 13s 41ms/sample - loss: 1.2692 - val_loss: 1.2105\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 13s 41ms/sample - loss: 1.2553 - val_loss: 1.2016\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 13s 39ms/sample - loss: 1.2423 - val_loss: 1.1947\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 13s 39ms/sample - loss: 1.2334 - val_loss: 1.1860\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.2248 - val_loss: 1.1815\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.2144 - val_loss: 1.1721\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 16s 49ms/sample - loss: 8.6682 - val_loss: 3.7024\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 2.8045 - val_loss: 2.1890\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.9644 - val_loss: 1.6998\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.7201 - val_loss: 1.5932\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.6105 - val_loss: 1.5387\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.5520 - val_loss: 1.4869\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.5023 - val_loss: 1.4406\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.4597 - val_loss: 1.4057\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.4222 - val_loss: 1.3696\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3891 - val_loss: 1.3430\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3624 - val_loss: 1.3224\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.3405 - val_loss: 1.3055\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3214 - val_loss: 1.2867\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3067 - val_loss: 1.2756\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2926 - val_loss: 1.2695\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2807 - val_loss: 1.2512\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2682 - val_loss: 1.2384\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.2586 - val_loss: 1.2442\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.2501 - val_loss: 1.2237\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2413 - val_loss: 1.2251\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 17s 51ms/sample - loss: 11.2078 - val_loss: 4.7070\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 3.2554 - val_loss: 2.8199\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 2.2224 - val_loss: 1.9888\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.7622 - val_loss: 1.7400\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 15s 47ms/sample - loss: 1.6200 - val_loss: 1.6845\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 15s 47ms/sample - loss: 1.5510 - val_loss: 1.6135\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 16s 47ms/sample - loss: 1.4984 - val_loss: 1.5677\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.4519 - val_loss: 1.5214\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.4112 - val_loss: 1.4808\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3745 - val_loss: 1.4431\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3407 - val_loss: 1.4091\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3109 - val_loss: 1.3815\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2851 - val_loss: 1.3577\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2639 - val_loss: 1.3391\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2485 - val_loss: 1.3242\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.2364 - val_loss: 1.3105\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.2258 - val_loss: 1.3019\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2171 - val_loss: 1.2880\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.2118 - val_loss: 1.2825\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 13s 41ms/sample - loss: 1.2064 - val_loss: 1.2774\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 17s 50ms/sample - loss: 8.8114 - val_loss: 4.3542\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 2.8587 - val_loss: 2.3386\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.9922 - val_loss: 1.7846\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.7241 - val_loss: 1.6904\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.6071 - val_loss: 1.6213\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.5290 - val_loss: 1.5367\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.4646 - val_loss: 1.4780\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.4129 - val_loss: 1.4318\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.3761 - val_loss: 1.4000\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.3464 - val_loss: 1.3809\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.3255 - val_loss: 1.3548\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3067 - val_loss: 1.3347\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2913 - val_loss: 1.3183\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2788 - val_loss: 1.3030\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.2685 - val_loss: 1.2897\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.2596 - val_loss: 1.2793\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2489 - val_loss: 1.2820\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2405 - val_loss: 1.2644\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2312 - val_loss: 1.2581\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.2213 - val_loss: 1.2384\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 17s 50ms/sample - loss: 13.2407 - val_loss: 5.0189\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 3.5273 - val_loss: 2.8402\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 2.2138 - val_loss: 1.9427\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.7613 - val_loss: 1.7090\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.6534 - val_loss: 1.6104\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.5734 - val_loss: 1.5428\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.5223 - val_loss: 1.4880\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.4786 - val_loss: 1.4540\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.4448 - val_loss: 1.4196\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.4091 - val_loss: 1.3894\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 13s 40ms/sample - loss: 1.3803 - val_loss: 1.3636\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 14s 41ms/sample - loss: 1.3551 - val_loss: 1.3418\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3331 - val_loss: 1.3283\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3130 - val_loss: 1.3038\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2939 - val_loss: 1.2868\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2788 - val_loss: 1.2742\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2661 - val_loss: 1.2670\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2559 - val_loss: 1.2515\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.2479 - val_loss: 1.2439\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 16s 47ms/sample - loss: 1.2369 - val_loss: 1.2378\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 19s 56ms/sample - loss: 9.3101 - val_loss: 4.8556\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 15s 47ms/sample - loss: 3.1170 - val_loss: 2.5171\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 2.1321 - val_loss: 1.8298\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.7956 - val_loss: 1.6971\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.6756 - val_loss: 1.6209\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.5841 - val_loss: 1.5374\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.5213 - val_loss: 1.4869\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.4769 - val_loss: 1.4421\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.4365 - val_loss: 1.4096\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.4029 - val_loss: 1.3765\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3751 - val_loss: 1.3507\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.3523 - val_loss: 1.3284\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.3346 - val_loss: 1.3087\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.3165 - val_loss: 1.2910\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3021 - val_loss: 1.2752\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2904 - val_loss: 1.2609\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.2757 - val_loss: 1.2499\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2656 - val_loss: 1.2389\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2561 - val_loss: 1.2292\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2453 - val_loss: 1.2191\n",
      "Train on 331 samples, validate on 37 samples\n",
      "Epoch 1/20\n",
      "331/331 [==============================] - 17s 52ms/sample - loss: 9.2662 - val_loss: 4.0254\n",
      "Epoch 2/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 2.9080 - val_loss: 2.2373\n",
      "Epoch 3/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 2.0523 - val_loss: 1.8658\n",
      "Epoch 4/20\n",
      "331/331 [==============================] - 16s 47ms/sample - loss: 1.7965 - val_loss: 1.6973\n",
      "Epoch 5/20\n",
      "331/331 [==============================] - 15s 47ms/sample - loss: 1.6537 - val_loss: 1.6145\n",
      "Epoch 6/20\n",
      "331/331 [==============================] - 16s 48ms/sample - loss: 1.5777 - val_loss: 1.5416\n",
      "Epoch 7/20\n",
      "331/331 [==============================] - 16s 47ms/sample - loss: 1.5183 - val_loss: 1.4963\n",
      "Epoch 8/20\n",
      "331/331 [==============================] - 15s 46ms/sample - loss: 1.4706 - val_loss: 1.4598\n",
      "Epoch 9/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.4313 - val_loss: 1.4256\n",
      "Epoch 10/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.3968 - val_loss: 1.3963\n",
      "Epoch 11/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3678 - val_loss: 1.3702\n",
      "Epoch 12/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.3433 - val_loss: 1.3549\n",
      "Epoch 13/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3192 - val_loss: 1.3359\n",
      "Epoch 14/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.3019 - val_loss: 1.3148\n",
      "Epoch 15/20\n",
      "331/331 [==============================] - 14s 44ms/sample - loss: 1.2840 - val_loss: 1.2984\n",
      "Epoch 16/20\n",
      "331/331 [==============================] - 15s 44ms/sample - loss: 1.2694 - val_loss: 1.2848\n",
      "Epoch 17/20\n",
      "331/331 [==============================] - 15s 45ms/sample - loss: 1.2547 - val_loss: 1.2717\n",
      "Epoch 18/20\n",
      "331/331 [==============================] - 14s 43ms/sample - loss: 1.2411 - val_loss: 1.2692\n",
      "Epoch 19/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2315 - val_loss: 1.2539\n",
      "Epoch 20/20\n",
      "331/331 [==============================] - 14s 42ms/sample - loss: 1.2201 - val_loss: 1.2434\n"
     ]
    }
   ],
   "source": [
    "sample_list = [i for i in range(len(X1))]\n",
    "rmse_z = []\n",
    "rmse_t = []\n",
    "ens_rmse_z = []\n",
    "ens_rmse_t = []\n",
    "import random\n",
    "\n",
    "ens_total = 0\n",
    "counter = 0\n",
    "\n",
    "for j in range(12):\n",
    "    random_list = random.sample(sample_list, 368)\n",
    "\n",
    "\n",
    "    X_train =np.ndarray(shape=(368, 32, 64, 2),dtype=np.float32)\n",
    "    y_train =np.ndarray(shape=(368, 32, 64, 2),dtype=np.float32)\n",
    "\n",
    "    for i in range(len(random_list)):\n",
    "        idx = random_list[i]\n",
    "        X_train[i, ...] = X1[idx]\n",
    "        y_train[i, ...] = y1[idx]\n",
    "    \n",
    "    cnn = build_cnn([64, 64, 64, 64, 2], [5, 3, 3, 3, 3], (32, 64, 2))\n",
    "    cnn.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "    \n",
    "    cnn.fit(x = X_train, y = y_train, epochs=20, validation_split=0.1, shuffle = True,\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=2,\n",
    "                        verbose=1, \n",
    "                        mode='auto'\n",
    "                    )]\n",
    "         )\n",
    "    \n",
    "    print('predicting')\n",
    "    fc = create_predictions(cnn, dg_test)\n",
    "    \n",
    "    ens = fc.copy()\n",
    "    \n",
    "    ens_total += ens\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    cnn_rmse = compute_weighted_rmse(fc, valid).compute()\n",
    "    \n",
    "    rmse_z.append(cnn_rmse.z)\n",
    "    rmse_t.append(cnn_rmse.t)\n",
    "    \n",
    "    cnn_rmse_ensemble = compute_weighted_rmse(ens_total/counter, valid)\n",
    "    \n",
    "    ens_rmse_z.append(cnn_rmse_ensemble.z)\n",
    "    ens_rmse_t.append(cnn_rmse_ensemble.t)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ens_num = [i for i in range(len(rmse_z))]\n",
    "plt.scatter(ens_num, rmse_z)\n",
    "plt.plot(ens_num, ens_rmse_z)\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(ens_num, rmse_t)\n",
    "plt.plot(ens_num, ens_rmse_t)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6 (test1)",
   "language": "python",
   "name": "python3_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
