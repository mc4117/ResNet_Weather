{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script name  /rds/general/user/mc4117/home/anaconda3/envs/test1/lib/python3.6/site-packages/ipykernel_launcher.py\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from src.score import *\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "\n",
    "import sys\n",
    "print(\"Script name \", sys.argv[0])\n",
    "\n",
    "block_no = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\"\"\"\n",
    " \n",
    "DATADIR = '/rds/general/user/mc4117/home/WeatherBench/data/'\n",
    "\n",
    "z500_valid = load_test_data(f'{DATADIR}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{DATADIR}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])\n",
    "\n",
    "z = xr.open_mfdataset(f'{DATADIR}geopotential_500/*.nc', combine='by_coords')\n",
    "t = xr.open_mfdataset(f'{DATADIR}temperature_850/*.nc', combine='by_coords').drop('level')\n",
    "\n",
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "datasets = [z, t]\n",
    "ds = xr.merge(datasets)\n",
    "\n",
    "# In this notebook let's only load a subset of the training data\n",
    "ds_train = ds.sel(time=slice('1979', '2016'))  \n",
    "ds_test = ds.sel(time=slice('2017', '2018'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, mean=None, std=None, bins_z = None, bins_t = None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for var, levels in var_dict.items():\n",
    "            try:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "            except ValueError:\n",
    "                data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "        self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "        \n",
    "        \n",
    "        self.bins_z = np.linspace(ds.z.min(), ds.z.max(), 100) if bins_z is None else bins_z\n",
    "        self.bins_t = np.linspace(ds.t.min(), ds.t.max(), 100) if bins_t is None else bins_t\n",
    "        \n",
    "        digitized_z_data = xr.DataArray(np.digitize(ds.z, self.bins_z)-1, dims=['time', 'lat', 'lon'], coords={'time':self.data.time.values, 'lat': self.data.lat.values, 'lon': self.data.lon.values})\n",
    "        \n",
    "        digitized_t_data = xr.DataArray(np.digitize(ds.t, self.bins_t)-1, dims=['time', 'lat', 'lon'], coords={'time':self.data.time.values, 'lat': self.data.lat.values, 'lon': self.data.lon.values})        \n",
    "        self.binned_data = xr.DataArray([digitized_z_data, digitized_t_data], dims=['var', 'time', 'lat', 'lon'], coords={'var': ['z', 't'], 'time':ds.time.values, 'lat': ds.lat.values, 'lon': ds.lon.values}).transpose('time', 'lat', 'lon', 'var')        \n",
    "\n",
    "        del digitized_z_data\n",
    "        del digitized_t_data\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "        if load: print('Loading data into RAM'); self.binned_data.load()            \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.binned_data.isel(time=idxs + self.lead_time).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({'z': None, 't': None})\n",
    "\n",
    "bs=32\n",
    "lead_time=72\n",
    "\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(\n",
    "    ds_train.sel(time=slice('1979', '2015')), dic, lead_time, batch_size=bs, load=True)\n",
    "\n",
    "dg_valid = DataGenerator(\n",
    "    ds_train.sel(time=slice('2016', '2016')), dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False, bins_z = dg_train.bins_z, bins_t = dg_train.bins_t)\n",
    "\n",
    "dg_test = DataGenerator(\n",
    "    ds_test, dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False, bins_z = dg_train.bins_z, bins_t = dg_train.bins_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'pad_width': self.pad_width})\n",
    "        return config\n",
    "\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 conv_kwargs={},\n",
    "                 **kwargs, ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        if type(kernel_size) is not int:\n",
    "            assert kernel_size[0] == kernel_size[1], 'PeriodicConv2D only works for square kernels'\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        self.conv = Conv2D(\n",
    "            filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'filters': self.filters, 'kernel_size': self.kernel_size, 'conv_kwargs': self.conv_kwargs})\n",
    "        return config\n",
    "    \n",
    "def convblock(inputs, f, k, l2, dr = 0):\n",
    "    x = inputs\n",
    "    if l2 is not None:\n",
    "        x = PeriodicConv2D(f, k, conv_kwargs={\n",
    "            'kernel_regularizer': keras.regularizers.l2(l2)})(x) \n",
    "    else:\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if dr>0: x = Dropout(dr)(x, training = True)\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_resnet_cnn(filters, kernels, input_shape, l2 = None, dr = 0, skip = True):\n",
    "    \"\"\"Fully convolutional residual network\"\"\"\n",
    "\n",
    "    x = input = Input(shape=input_shape)\n",
    "    x = convblock(x, filters[0], kernels[0], dr)\n",
    "\n",
    "    #Residual blocks\n",
    "    for f, k in zip(filters[1:-1], kernels[1:-1]):\n",
    "        y = x\n",
    "        for _ in range(2):\n",
    "            x = convblock(x, f, k, l2, dr)\n",
    "        if skip: x = Add()([y, x])\n",
    "\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    \n",
    "    return keras.models.Model(input, output)\n",
    "\n",
    "filt = [64]\n",
    "kern = [5]\n",
    "\n",
    "for i in range(int(block_no)):\n",
    "    filt.append(64)\n",
    "    kern.append(5)\n",
    "\n",
    "filt.append(2)\n",
    "kern.append(5)\n",
    "\n",
    "cnn = build_resnet_cnn(filt, kern, (32, 64, 2), l2 = 1e-5)\n",
    "\n",
    "cnn.compile(keras.optimizers.Adam(5e-5), 'mse')\n",
    "\n",
    "\n",
    "print(cnn.summary())\n",
    "\"\"\"\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=5,\n",
    "                        verbose=1, \n",
    "                        mode='auto'\n",
    "                    )\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = 'val_loss',\n",
    "            patience=2,\n",
    "            factor=0.2,\n",
    "            verbose=1)\n",
    "\n",
    "\n",
    "cnn.fit(dg_train, epochs=100, validation_data=dg_valid, \n",
    "          callbacks=[early_stopping_callback, reduce_lr_callback]\n",
    "         )\n",
    "\"\"\"\n",
    "\n",
    "cnn.load_weights('/rds/general/user/mc4117/home/WeatherBench/saved_models/whole_cat_both_no_ohe_' + str(block_no) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnn.predict(dg_test)\n",
    "\n",
    "preds_cat = xr.DataArray(output[:, :, :, 0], dims=['time', 'lat', 'lon'], coords={'time': dg_test.valid_time, 'lat': dg_test.ds.lat, 'lon': dg_test.ds.lon}) \n",
    "\n",
    "preds_cat_int = np.round(preds_cat, 0)\n",
    "\n",
    "nx, ny, nz = preds_cat.shape\n",
    "\n",
    "preds_cat_inv = np.zeros((nx, ny, nz))\n",
    "\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        for k in range(nz):\n",
    "            preds_cat_inv[i, j, k] = dg_test.bins_z[int(preds_cat_int[i, j, k])]\n",
    "\n",
    "full_out_ds = xr.DataArray(\n",
    "        preds_cat_inv,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon})\n",
    "\n",
    "print(compute_weighted_rmse(full_out_ds, z500_valid[72:]).compute())\n",
    "\n",
    "\n",
    "preds_cat_t = xr.DataArray(output[:, :, :, 1], dims=['time', 'lat', 'lon'], coords={'time': dg_test.valid_time, 'lat': dg_test.ds.lat, 'lon': dg_test.ds.lon})\n",
    "\n",
    "preds_cat_int_t = np.round(preds_cat_t, 0)\n",
    "\n",
    "nx, ny, nz = preds_cat_t.shape\n",
    "\n",
    "preds_cat_inv_t = np.zeros((nx, ny, nz))\n",
    "\n",
    "for i in range(nx):\n",
    "    for j in range(ny):\n",
    "        for k in range(nz):\n",
    "            preds_cat_inv_t[i, j, k] = dg_test.bins_t[int(preds_cat_int_t[i, j, k])]\n",
    "\n",
    "full_out_ds_t = xr.DataArray(\n",
    "        preds_cat_inv_t,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon})\n",
    "\n",
    "print(compute_weighted_rmse(full_out_ds, t850_valid[72:]).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = dg_test[0]\n",
    "\n",
    "for i in range(1, len(dg_test)):\n",
    "    X2, y2 = dg_test[i]\n",
    "    y1 = np.concatenate((y1, y2))\n",
    "\n",
    "nx_y, ny_y, nz_y = y1.shape\n",
    "\n",
    "y1_full = np.zeros((nx_y, ny_y, nz_y))\n",
    "\n",
    "for i in range(nx_y):\n",
    "    for j in range(ny_y):\n",
    "        for k in range(nz_y):\n",
    "            y1_full[i][j][k] = dg_test.bins_z[y1[i][j][k].argmax()]\n",
    "\n",
    "import ipdb; ipdb.set_trace()\n",
    "\n",
    "real_ds = xr.DataArray(\n",
    "        y1_full[:, :, :, 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon})\n",
    "\n",
    "print(compute_weighted_rmse(full_out_ds, real_ds).compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test1]",
   "language": "python",
   "name": "conda-env-test1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
