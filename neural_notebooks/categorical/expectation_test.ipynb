{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data \n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import itertools\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from src.score import *\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "DATADIR = '/rds/general/user/mc4117/home/WeatherBench/data/'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "if var_name == 'specific_humidity':\n",
    "    if args.level_list is None:\n",
    "        unique_list = [150, 200, 600, 700, 850, 925, 1000]\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        'specific_humidity': ('q', unique_list)}\n",
    "elif var_name == '2m_temp':\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        '2m_temperature': ('t2m', None)}\n",
    "elif var_name == 'solar_rad':\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        'toa_incident_solar_radiation': ('tisr', None)}\n",
    "elif var_name == 'pot_vort':\n",
    "    if args.level_list is None:\n",
    "        unique_list = [150, 250, 300, 700, 850]\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        'potential_vorticity': ('pv', unique_list)}\n",
    "elif var_name == 'const':\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        'constants': ['lat2d', 'orography', 'lsm']}\n",
    "elif var_name == 'orig':\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850])} \n",
    "elif var_name == 'temp':\n",
    "    if args.level_list is None:\n",
    "        unique_list = [300, 400, 500, 600, 700, 850]\n",
    "    else:\n",
    "        unique_list.append(850)\n",
    "        unique_list = sorted(list(dict.fromkeys(unique_list)))\n",
    "    print(unique_list)\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', unique_list)}\n",
    "elif var_name == 'geo':\n",
    "    if args.level_list is None:\n",
    "        unique_list = [300, 400, 500, 600, 700, 850]\n",
    "    else:\n",
    "        unique_list.append(500)\n",
    "        unique_list = sorted(list(dict.fromkeys(unique_list)))\n",
    "    print(unique_list)\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', unique_list),\n",
    "        'temperature': ('t', [850])} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "var_dict = {\n",
    "    'geopotential': ('z', [500]),\n",
    "    'temperature': ('t', [850])}   \n",
    "\n",
    "ds_list = []\n",
    "\n",
    "for long_var, params in var_dict.items():\n",
    "    if long_var == 'constants':\n",
    "        ds_list.append(xr.open_mfdataset(f'{DATADIR}/{long_var}/*.nc', combine='by_coords'))\n",
    "    else:\n",
    "        var, levels = params\n",
    "        if levels is not None:\n",
    "            ds_list.append(xr.open_mfdataset(f'{DATADIR}/{long_var}/*.nc', combine='by_coords').sel(level = levels))\n",
    "        else:\n",
    "            ds_list.append(xr.open_mfdataset(f'{DATADIR}/{long_var}/*.nc', combine='by_coords'))\n",
    "\n",
    "# because missing first values of solar radiation exclude these from the dataset\n",
    "ds_whole = xr.merge(ds_list).isel(time = slice(7, None))\n",
    "\n",
    "ds_train = ds_whole.sel(time=slice('1979', '2016'))  \n",
    "ds_test = ds_whole.sel(time=slice('2017', '2018'))\n",
    "ds_valid = ds_whole.sel(time=slice('2015', '2016'))\n",
    "\n",
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, \n",
    "                 mean=None, std=None, output_vars= None, bins_z = None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        level_names = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for long_var, params in var_dict.items():\n",
    "            if long_var == 'constants': \n",
    "                for var in params:\n",
    "                    data.append(ds[var].expand_dims(\n",
    "                        {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "                    ))\n",
    "                    level_names.append(var)\n",
    "            else:\n",
    "                var, levels = params\n",
    "                try:\n",
    "                    data.append(ds[var].sel(level=levels))\n",
    "                    level_names += [f'{var}_{level}' for level in levels]\n",
    "                except ValueError:\n",
    "                    data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "                    level_names.append(var)   \n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.data['level_names'] = xr.DataArray(\n",
    "            level_names, dims=['level'], coords={'level': self.data.level})\n",
    "        if output_vars is None:\n",
    "            self.output_idxs = range(len(ds.data.level))\n",
    "        else:\n",
    "            self.output_idxs = [i for i, l in enumerate(self.data.level_names.values) \n",
    "                                if any([bool(re.match(o, l)) for o in output_vars])]\n",
    "\n",
    "        self.bins_z = np.linspace(self.data.isel(level =self.output_idxs).min(), self.data.isel(level =self.output_idxs).max(), 100) if bins_z is None else bins_z \n",
    "        self.binned_data = xr.DataArray((np.digitize(self.data.isel(level=self.output_idxs), self.bins_z)-1)[:,:,:,0], dims=['time', 'lat', 'lon'], coords={'time':self.data.time.values, 'lat': self.data.lat.values, 'lon': self.data.lon.values})\n",
    "        \n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "        self.std = self.data.std(('time', 'lat', 'lon')).compute() if std is None else std\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        del ds\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "        if load: print('Loading data into RAM'); self.binned_data.load()            \n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.binned_data.isel(time=idxs + self.lead_time).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)\n",
    "\n",
    "bs=32\n",
    "lead_time=72\n",
    "output_vars = ['z_500']\n",
    "\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(\n",
    "    ds_train.sel(time=slice('1979', '2013')), var_dict, lead_time, batch_size=bs, load=True, output_vars = output_vars)\n",
    "\n",
    "dg_valid = DataGenerator(\n",
    "    ds_train.sel(time=slice('2015', '2016')), var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, bins_z = dg_train.bins_z, shuffle=False, output_vars = output_vars)\n",
    "\n",
    "\"\"\"\n",
    "dg_valid2 = DataGenerator(\n",
    "    ds_train.sel(time=slice('2014', '2014')), var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, bins_z = dg_train.bins_z, shuffle=False, output_vars = output_vars)\n",
    "\"\"\"\n",
    "dg_test = DataGenerator(\n",
    "    ds_test, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, bins_z = dg_train.bins_z, shuffle=False, output_vars = output_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_no = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stack = np.load('test_stack.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded = np.round(test_stack[:,:,:,0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_z_avg = [(dg_test.bins_z[i] + dg_test.bins_z[i+1])/2 for i in range(len(dg_test.bins_z)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Coordinates:\n",
      "    level    int64 500\n",
      "Data variables:\n",
      "    z        float64 450.7\n"
     ]
    }
   ],
   "source": [
    "for i in range(99):\n",
    "    rounded[rounded == i] = bins_z_avg[i]\n",
    "\n",
    "fc_conv_ds_avg = xr.Dataset({\n",
    "        'z': xr.DataArray(\n",
    "              rounded,\n",
    "               dims=['time', 'lat', 'lon'],\n",
    "               coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                })})\n",
    "    \n",
    "cnn_rmse_arg = compute_weighted_rmse(fc_conv_ds_avg, ds_test.z.sel(level = 500)[72:]).compute()\n",
    "print(cnn_rmse_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50560.321812880444"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dg_test.bins_z[47] + (0.690666*(dg_test.bins_z[48]-dg_test.bins_z[47]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_diff = (test_stack[:,:,:,0]-np.round(test_stack[:,:,:,0], 0))*(dg_test.bins_z[1] - dg_test.bins_z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded = np.round(test_stack[:,:,:,0], 0)\n",
    "for i in range(99):\n",
    "    rounded[rounded == i] = bins_z_avg[i]\n",
    "    \n",
    "rounded_final = rounded+rounded_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Coordinates:\n",
      "    level    int64 500\n",
      "Data variables:\n",
      "    z        float64 448.4\n"
     ]
    }
   ],
   "source": [
    "fc_conv_ds_avg = xr.Dataset({\n",
    "        'z': xr.DataArray(\n",
    "              rounded_final,\n",
    "               dims=['time', 'lat', 'lon'],\n",
    "               coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                })})\n",
    "    \n",
    "cnn_rmse_arg = compute_weighted_rmse(fc_conv_ds_avg, ds_test.z.sel(level = 500)[72:]).compute()\n",
    "print(cnn_rmse_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 26.6 GiB for an array with shape (17448, 32, 64, 100) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-41213c87aa5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_test_geo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_no\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_geo_[300, 400, 500, 600, 700, 850]_preds_cat_test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbin_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 26.6 GiB for an array with shape (17448, 32, 64, 100) and data type float64"
     ]
    }
   ],
   "source": [
    "output_test_geo = np.expand_dims(np.dot(np.load('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/' + str(block_no) + '_geo_[300, 400, 500, 600, 700, 850]_preds_cat_test.npy'), bin_values), axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_geo = xr.Dataset({\n",
    "        'z': xr.DataArray(\n",
    "              output_test_geo[:,:,:,0],\n",
    "               dims=['time', 'lat', 'lon'],\n",
    "               coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                })})\n",
    "    \n",
    "cnn_rmse_arg = compute_weighted_rmse(fc_geo, ds_test.z.sel(level = 500)[72:]).compute()\n",
    "print(cnn_rmse_arg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test2]",
   "language": "python",
   "name": "conda-env-test2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
