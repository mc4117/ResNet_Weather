{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train a CNN\n",
    "\n",
    "Add more data but actually get more error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Depending on your combination of package versions, this can raise a lot of TF warnings... \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from src.score import *\n",
    "from collections import OrderedDict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "DATADIR = '/rds/general/user/mc4117/home/WeatherBench/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create data generator\n",
    "\n",
    "First up, we want to write our own Keras data generator. The key advantage to just feeding in numpy arrays is that we don't have to load the data twice because our intputs and outputs are the same data just offset by the lead time. Since the dataset is quite large and we might run out of CPU RAM this is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "(32, 32, 64, 10)\n",
      "(32, 32, 64, 2)\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, \n",
    "                 mean=None, std=None, output_vars=None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        level_names = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for long_var, params in var_dict.items():\n",
    "            if long_var == 'constants': \n",
    "                for var in params:\n",
    "                    data.append(ds[var].expand_dims(\n",
    "                        {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "                    ))\n",
    "                    level_names.append(var)\n",
    "            else:\n",
    "                var, levels = params\n",
    "                try:\n",
    "                    data.append(ds[var].sel(level=levels))\n",
    "                    level_names += [f'{var}_{level}' for level in levels]\n",
    "                except ValueError:\n",
    "                    data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "                    level_names.append(var)\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.data['level_names'] = xr.DataArray(\n",
    "            level_names, dims=['level'], coords={'level': self.data.level})\n",
    "        if output_vars is None:\n",
    "            self.output_idxs = range(len(dg_valid.data.level))\n",
    "        else:\n",
    "            self.output_idxs = [i for i, l in enumerate(self.data.level_names.values) \n",
    "                                if any([bool(re.match(o, l)) for o in output_vars])]\n",
    "        \n",
    "        # Normalize\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "#         self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        self.std = self.data.std(('time', 'lat', 'lon')).compute() if std is None else std\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time, level=self.output_idxs).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)\n",
    "\n",
    "var_dict = {\n",
    "    'geopotential': ('z', [500, 850]),\n",
    "    'temperature': ('t', [500, 850]),\n",
    "    'specific_humidity': ('q', [850]),\n",
    "    '2m_temperature': ('t2m', None),\n",
    "    'potential_vorticity': ('pv', [50, 100]),\n",
    "    'constants': ['lsm', 'orography']\n",
    "}\n",
    "\n",
    "ds = [xr.open_mfdataset(f'{DATADIR}/{var}/*.nc', combine='by_coords') for var in var_dict.keys()]\n",
    "\n",
    "ds_whole = xr.merge(ds)\n",
    "\n",
    "ds_train = ds_whole.sel(time=slice('2014', '2015'))\n",
    "ds_valid = ds_whole.sel(time=slice('2016', '2016'))\n",
    "ds_test = ds_whole.sel(time=slice('2017', '2018'))\n",
    "\n",
    "bs=32\n",
    "lead_time=72\n",
    "output_vars = ['z_500', 't_850']\n",
    "\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(ds_train, var_dict, lead_time, batch_size=bs, load=True, \n",
    "                         output_vars=output_vars)\n",
    "dg_valid = DataGenerator(ds_valid, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, \n",
    "                         shuffle=False, output_vars=output_vars)\n",
    "\n",
    "dg_test = DataGenerator(ds_test, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, \n",
    "                         shuffle=False, output_vars=output_vars)\n",
    "\n",
    "X, y = dg_train[0]; \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'pad_width': self.pad_width})\n",
    "        return config\n",
    "\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 conv_kwargs={},\n",
    "                 **kwargs, ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        if type(kernel_size) is not int:\n",
    "            assert kernel_size[0] == kernel_size[1], 'PeriodicConv2D only works for square kernels'\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        self.conv = Conv2D(\n",
    "            filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'filters': self.filters, 'kernel_size': self.kernel_size, 'conv_kwargs': self.conv_kwargs})\n",
    "        return config\n",
    "    \n",
    "def build_cnn(filters, kernels, input_shape, dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        if dr > 0: x = Dropout(dr)(x, training = True)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)\n",
    "\n",
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create non-iterative predictions\"\"\"\n",
    "    preds = xr.DataArray(\n",
    "        model.predict_generator(dg),\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg.valid_time, 'lat': dg.data.lat, 'lon': dg.data.lon, \n",
    "                'level': dg.data.isel(level=dg.output_idxs).level,\n",
    "                'level_names': dg.data.isel(level=dg.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "    # Unnormalize\n",
    "    preds = (preds * dg.std.isel(level=dg.output_idxs).values + \n",
    "             dg.mean.isel(level=dg.output_idxs).values)\n",
    "    unique_vars = list(set([l.split('_')[0] for l in preds.level_names.values])); unique_vars\n",
    "    \n",
    "    das = []\n",
    "    for v in unique_vars:\n",
    "        idxs = [i for i, vv in enumerate(preds.level_names.values) if vv.split('_')[0] in v]\n",
    "        #print(v, idxs)\n",
    "        da = preds.isel(level=idxs).squeeze().drop('level_names')\n",
    "        if not 'level' in da.dims: da.drop('level')\n",
    "        das.append({v: da})\n",
    "    return xr.merge(das, compat = 'override').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_1.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_1.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_0.h5')\n",
    "\n",
    "cnn_2 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_2.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_2.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_1.h5')\n",
    "\n",
    "cnn_3 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_3.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_3.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_2.h5')\n",
    "\n",
    "cnn_4 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_4.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_4.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_3.h5')\n",
    "\n",
    "cnn_5 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_5.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_5.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_4.h5')\n",
    "\n",
    "cnn_6 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_6.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_6.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_5.h5')\n",
    "\n",
    "cnn_7 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_7.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_7.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_6.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create non-iterative predictions\"\"\"\n",
    "    preds = xr.DataArray(\n",
    "        model.predict_generator(dg),\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg.valid_time, 'lat': dg.data.lat, 'lon': dg.data.lon, \n",
    "                'level': dg.data.isel(level=dg.output_idxs).level,\n",
    "                'level_names': dg.data.isel(level=dg.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "    # Don't unnormalize\n",
    "\n",
    "    unique_vars = list(set([l.split('_')[0] for l in preds.level_names.values])); unique_vars\n",
    "    \n",
    "    das = []\n",
    "    for v in unique_vars:\n",
    "        idxs = [i for i, vv in enumerate(preds.level_names.values) if vv.split('_')[0] in v]\n",
    "        #print(v, idxs)\n",
    "        da = preds.isel(level=idxs).squeeze().drop('level_names')\n",
    "        if not 'level' in da.dims: da.drop('level')\n",
    "        das.append({v: da})\n",
    "    return xr.merge(das, compat = 'override').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_1 = create_predictions(cnn_1, dg_test)\n",
    "fc_2 = create_predictions(cnn_2, dg_test)\n",
    "fc_3 = create_predictions(cnn_3, dg_test)\n",
    "fc_4 = create_predictions(cnn_4, dg_test)\n",
    "fc_5 = create_predictions(cnn_5, dg_test)\n",
    "fc_6 = create_predictions(cnn_6, dg_test)\n",
    "fc_7 = create_predictions(cnn_7, dg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.stack((fc_1.to_array(), fc_2.to_array(), fc_3.to_array(), fc_4.to_array(), fc_5.to_array(), fc_6.to_array(), fc_7.to_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 2, 17448, 32, 64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_stack(filters, kernels, input_shape, dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        #x = BatchNormalization()(x)\n",
    "        if dr > 0: x = Dropout(dr)(x, training = True)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = dg_test[0]\n",
    "\n",
    "for i in range(1, len(dg_test)):\n",
    "    X2, y2 = dg_test[i]\n",
    "    y1 = np.concatenate((y1, y2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17448, 32, 64, 7, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_rearranged = np.transpose(preds, axes = (2, 3, 4, 0, 1))\n",
    "preds_rearranged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17448, 32, 64, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 32, 64, 7)]       0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_55 (Periodic (None, 32, 64, 64)        11264     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)   (None, 32, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 32, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_56 (Periodic (None, 32, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)   (None, 32, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 32, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_57 (Periodic (None, 32, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)   (None, 32, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 32, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_58 (Periodic (None, 32, 64, 64)        102464    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, 32, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 32, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "periodic_conv2d_59 (Periodic (None, 32, 64, 1)         1601      \n",
      "=================================================================\n",
      "Total params: 320,257\n",
      "Trainable params: 320,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cnn = build_cnn_stack([64, 64, 64, 64, 1], [5, 5, 5, 5, 5], (32, 64, 7), dr = 0.1)\n",
    "\n",
    "cnn.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "\n",
    "print(cnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13958 samples, validate on 3490 samples\n",
      "Epoch 1/10\n",
      " 1824/13958 [==>...........................] - ETA: 9:45 - loss: 0.1558"
     ]
    }
   ],
   "source": [
    "cnn.fit(x= preds_rearranged[..., 0], y= y1[..., 0], shuffle = True, epochs = 10, validation_split = 0.2,\n",
    "          callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=1,\n",
    "                        verbose=1, \n",
    "                        mode='auto'\n",
    "                    )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save_weights('/rds/general/user/mc4117/ephemeral/saved_models/stack.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_valid_2 = ds_whole.sel(time=slice('2013', '2014'))\n",
    "\n",
    "dg_valid_2 = DataGenerator(ds_valid_2, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, \n",
    "                         shuffle=False, output_vars=output_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_1_valid = create_predictions(cnn_1, dg_valid_2)\n",
    "fc_2_valid = create_predictions(cnn_2, dg_valid_2)\n",
    "fc_3 = create_predictions(cnn_3, dg_valid_2)\n",
    "fc_4 = create_predictions(cnn_4, dg_valid_2)\n",
    "fc_5 = create_predictions(cnn_5, dg_valid_2)\n",
    "fc_6 = create_predictions(cnn_6, dg_valid_2)\n",
    "fc_7 = create_predictions(cnn_7, dg_valid_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_valid = np.stack((fc_1.to_array(), fc_2.to_array(), fc_3.to_array(), fc_4.to_array(), fc_5.to_array(), fc_6.to_array(), fc_7.to_array()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_valid_2, y1_valid_2 = dg_valid_2[0]\n",
    "\n",
    "for i in range(1, len(dg_valid_2)):\n",
    "    X2_valid_2, y2_valid_2 = dg_valid_2[i]\n",
    "    y1_valid_2 = np.concatenate((y1_valid_2, y2_valid_2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rearranged = np.transpose(preds, axes = (2, 3, 4, 0, 1))\n",
    "preds_rearranged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pred_stack = cnn.predict(preds_rearranged[..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pred_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stack = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        arr_pred_stack[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid_2.data.time[72:], 'lat': dg_valid_2.data.lat, 'lon': dg_valid_2.data.lon,\n",
    "                },\n",
    "    ),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real_unnorm =y1_valid_2* dg_valid_2.std.isel(level=[0]).values+dg_valid_2.mean.isel(level=[0]).values\n",
    "\n",
    "real_ds = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        y1_valid_2[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid_2.data.time[72:], 'lat': dg_valid_2.data.lat, 'lon': dg_valid_2.data.lon,\n",
    "                },\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse(pred_stack, real_ds).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(compute_weighted_rmse(fc_1, real_ds).compute())\n",
    "print(compute_weighted_rmse(fc_2, real_ds).compute())\n",
    "print(compute_weighted_rmse(fc_3, real_ds).compute())\n",
    "print(compute_weighted_rmse(fc_4, real_ds).compute())\n",
    "print(compute_weighted_rmse(fc_5, real_ds).compute())\n",
    "print(compute_weighted_rmse(fc_6, real_ds).compute())\n",
    "print(compute_weighted_rmse(fc_7, real_ds).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_train = cnn.predict(preds_rearranged[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_ds_test = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        y1[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "pred_train = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        fc_train[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    ),})\n",
    "\n",
    "compute_weighted_rmse(fc_train[...,0], real_ds_test).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    pred0_init_train = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        preds[i, 0, ...],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    ),})\n",
    "\n",
    "    print(compute_weighted_rmse(pred0_init_train, real_ds_test).compute())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6 (test1)",
   "language": "python",
   "name": "python3_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
