{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Depending on your combination of package versions, this can raise a lot of TF warnings... \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from src.score import *\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def limit_mem():\n",
    "    \"\"\"By default TF uses all available GPU memory. This function prevents this.\"\"\"\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# GPU\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(0)\n",
    "limit_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "DATADIR = '/rds/general/user/mc4117/home/WeatherBench/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create data generator\n",
    "\n",
    "First up, we want to write our own Keras data generator. The key advantage to just feeding in numpy arrays is that we don't have to load the data twice because our intputs and outputs are the same data just offset by the lead time. Since the dataset is quite large and we might run out of CPU RAM this is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the validation subset of the data: 2017 and 2018\n",
    "z500_valid = load_test_data(f'{DATADIR}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{DATADIR}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = xr.open_mfdataset(f'{DATADIR}geopotential_500/*.nc', combine='by_coords')\n",
    "t = xr.open_mfdataset(f'{DATADIR}temperature_850/*.nc', combine='by_coords').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "datasets = [z, t]\n",
    "ds = xr.merge(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook let's only load a subset of the training data\n",
    "\n",
    "ds_2015 = ds.sel(time = '2015-06')\n",
    "ds_2016 = ds.sel(time = '2016-06')\n",
    "ds_2017 = ds.sel(time = '2017-06')\n",
    "ds_2018 = ds.sel(time = '2018-06')\n",
    "\n",
    "ds_train = ds_2015\n",
    "ds_valid = ds_2016\n",
    "ds_test = xr.merge([ds_2017, ds_2018])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, mean=None, std=None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for var, levels in var_dict.items():\n",
    "            try:\n",
    "                data.append(ds[var].sel(level=levels))\n",
    "            except ValueError:\n",
    "                data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "        self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        # Normalize\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we need a dictionary for all the variables and levels we want to extract from the dataset\n",
    "dic = OrderedDict({'z': None, 't': None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=32\n",
    "lead_time=72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(ds_train, dic, lead_time, batch_size=bs, load=True)\n",
    "dg_valid = DataGenerator(ds_valid, dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n"
     ]
    }
   ],
   "source": [
    "# Now also a generator for testing. Impartant: Shuffle must be False!\n",
    "dg_test = DataGenerator(ds_test, dic, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create and train model\n",
    "\n",
    "Next up, we need to create the model architecture. Here we will use a fully connected convolutional network. Because the Earth is periodic in longitude, we want to use a periodic convolution in the lon-direction. This is not implemented in Keras, so we have to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'pad_width': self.pad_width})\n",
    "        return config\n",
    "\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 conv_kwargs={},\n",
    "                 **kwargs, ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        if type(kernel_size) is not int:\n",
    "            assert kernel_size[0] == kernel_size[1], 'PeriodicConv2D only works for square kernels'\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        self.conv = Conv2D(\n",
    "            filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'filters': self.filters, 'kernel_size': self.kernel_size, 'conv_kwargs': self.conv_kwargs})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def convblock(inputs, f, k, l2, dr = 0):\n",
    "    x = inputs\n",
    "    if l2 is not None:\n",
    "        x = PeriodicConv2D(f, k, conv_kwargs={\n",
    "            'kernel_regularizer': keras.regularizers.l2(l2)})(x) \n",
    "    else:\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    if dr>0: x = Dropout(dr)(x, training = True)\n",
    "\n",
    "    return x\n",
    "\n",
    "def build_resnet_cnn(filters, kernels, input_shape, l2 = None, dr = 0, skip = True):\n",
    "    \"\"\"Fully convolutional residual network\"\"\"\n",
    "\n",
    "    x = input = Input(shape=input_shape)\n",
    "    x = convblock(x, filters[0], kernels[0], dr)\n",
    "\n",
    "    #Residual blocks\n",
    "    for f, k in zip(filters[1:-1], kernels[1:-1]):\n",
    "        y = x\n",
    "        for _ in range(2):\n",
    "            x = convblock(x, f, k, l2, dr)\n",
    "        if skip: x = Add()([y, x])\n",
    "\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    \n",
    "    return keras.models.Model(input, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cnn = build_resnet_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 2), l2 = 1e-5, dr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "cnn.compile(keras.optimizers.Adam(5e-5), 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 64, 2)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "periodic_conv2d (PeriodicConv2D (None, 32, 64, 64)   3264        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 32, 64, 64)   0           periodic_conv2d[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 64, 64)   256         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "periodic_conv2d_1 (PeriodicConv (None, 32, 64, 64)   102464      batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 64, 64)   0           periodic_conv2d_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 64, 64)   256         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 32, 64, 64)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "periodic_conv2d_2 (PeriodicConv (None, 32, 64, 64)   102464      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32, 64, 64)   0           periodic_conv2d_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 64, 64)   256         leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 32, 64, 64)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 64, 64)   0           batch_normalization[0][0]        \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "periodic_conv2d_3 (PeriodicConv (None, 32, 64, 64)   102464      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 32, 64, 64)   0           periodic_conv2d_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 64, 64)   256         leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 32, 64, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "periodic_conv2d_4 (PeriodicConv (None, 32, 64, 64)   102464      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32, 64, 64)   0           periodic_conv2d_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 64, 64)   256         leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 64, 64)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 64, 64)   0           add[0][0]                        \n",
      "                                                                 dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "periodic_conv2d_5 (PeriodicConv (None, 32, 64, 64)   102464      add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 32, 64, 64)   0           periodic_conv2d_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 64, 64)   256         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 32, 64, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "periodic_conv2d_6 (PeriodicConv (None, 32, 64, 64)   102464      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 64, 64)   0           periodic_conv2d_6[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 64, 64)   256         leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 32, 64, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 32, 64, 64)   0           add_1[0][0]                      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "periodic_conv2d_7 (PeriodicConv (None, 32, 64, 2)    3202        add_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 623,042\n",
      "Trainable params: 622,146\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 21 steps, validate for 21 steps\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - 170s 8s/step - loss: 10.5185 - val_loss: 6.1949\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - 163s 8s/step - loss: 4.1930 - val_loss: 4.3389\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - 188s 9s/step - loss: 3.5323 - val_loss: 4.0118\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - 190s 9s/step - loss: 3.2686 - val_loss: 3.7477\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - 191s 9s/step - loss: 3.0956 - val_loss: 3.5623\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - 180s 9s/step - loss: 2.9734 - val_loss: 3.4197\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - 183s 9s/step - loss: 2.8751 - val_loss: 3.3464\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - 173s 8s/step - loss: 2.8051 - val_loss: 3.2682\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - 172s 8s/step - loss: 2.7397 - val_loss: 3.2292\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - 159s 8s/step - loss: 2.6911 - val_loss: 3.1866\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - 171s 8s/step - loss: 2.6446 - val_loss: 3.1522\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - 169s 8s/step - loss: 2.6032 - val_loss: 3.1285\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - 164s 8s/step - loss: 2.5698 - val_loss: 3.0975\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - 163s 8s/step - loss: 2.5343 - val_loss: 3.0637\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - 170s 8s/step - loss: 2.5079 - val_loss: 3.0251\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - 168s 8s/step - loss: 2.4807 - val_loss: 2.9901\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - 175s 8s/step - loss: 2.4554 - val_loss: 2.9656\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - 178s 8s/step - loss: 2.4293 - val_loss: 2.9287\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - 181s 9s/step - loss: 2.4040 - val_loss: 2.8970\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - 180s 9s/step - loss: 2.3811 - val_loss: 2.8616\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - 165s 8s/step - loss: 2.3623 - val_loss: 2.8195\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - 185s 9s/step - loss: 2.3413 - val_loss: 2.7855\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - 181s 9s/step - loss: 2.3230 - val_loss: 2.7624\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - 172s 8s/step - loss: 2.3031 - val_loss: 2.7344\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - 152s 7s/step - loss: 2.2840 - val_loss: 2.7092\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - 235s 11s/step - loss: 2.2669 - val_loss: 2.6956\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - 227s 11s/step - loss: 2.2443 - val_loss: 2.6677\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - 248s 12s/step - loss: 2.2295 - val_loss: 2.6748\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - 283s 13s/step - loss: 2.2123 - val_loss: 2.6425\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - 294s 14s/step - loss: 2.1944 - val_loss: 2.6414\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - 276s 13s/step - loss: 2.1780 - val_loss: 2.6218\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - 153s 7s/step - loss: 2.1578 - val_loss: 2.5995\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - 134s 6s/step - loss: 2.1440 - val_loss: 2.5827\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - 132s 6s/step - loss: 2.1267 - val_loss: 2.6126\n",
      "Epoch 35/100\n",
      "20/21 [===========================>..] - ETA: 4s - loss: 2.1116\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "21/21 [==============================] - 126s 6s/step - loss: 2.1117 - val_loss: 2.5939\n",
      "Epoch 36/100\n",
      "21/21 [==============================] - 108s 5s/step - loss: 2.0975 - val_loss: 2.6006\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - 106s 5s/step - loss: 2.0948 - val_loss: 2.5816\n",
      "Epoch 38/100\n",
      "21/21 [==============================] - 101s 5s/step - loss: 2.0904 - val_loss: 2.5735\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - 99s 5s/step - loss: 2.0857 - val_loss: 2.5802\n",
      "Epoch 40/100\n",
      "20/21 [===========================>..] - ETA: 3s - loss: 2.0831\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
      "21/21 [==============================] - 102s 5s/step - loss: 2.0823 - val_loss: 2.5796\n",
      "Epoch 41/100\n",
      "21/21 [==============================] - 101s 5s/step - loss: 2.0804 - val_loss: 2.5792\n",
      "Epoch 42/100\n",
      "20/21 [===========================>..] - ETA: 3s - loss: 2.0796\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 3.999999989900971e-07.\n",
      "21/21 [==============================] - 107s 5s/step - loss: 2.0789 - val_loss: 2.5763\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - 103s 5s/step - loss: 2.0801 - val_loss: 2.5764\n",
      "Epoch 00043: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b18df58d208>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Since we didn't load the full data this is only for demonstration.\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=5,\n",
    "                        verbose=1, \n",
    "                        mode='auto'\n",
    "                    )\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = 'val_loss',\n",
    "            patience=2,\n",
    "            factor=0.2,\n",
    "            verbose=1)\n",
    "\n",
    "cnn.fit(dg_train, epochs=100, validation_data=dg_valid, \n",
    "          callbacks=[early_stopping_callback, reduce_lr_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create predictions\n",
    "\n",
    "Now that we have our model we need to create a prediction NetCDF file. This function does this. \n",
    "\n",
    "We can either directly predict the target lead time (e.g. 5 days) or create an iterative forecast by chaining together many e.g. 6h forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dg_test has length 43 (roughly number of samples/batch size)\n",
    "#X, y = dg_test[0]\n",
    "\n",
    "X1, y1 = dg_test[0]\n",
    "\n",
    "for i in range(1, len(dg_test)):\n",
    "    X2, y2 = dg_test[i]\n",
    "    X1 = np.concatenate((X1, X2))\n",
    "    y1 = np.concatenate((y1, y2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1368, 32, 64, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create predictions for non-iterative model\"\"\"\n",
    "    preds = model.predict(dg)\n",
    "    # Unnormalize\n",
    "    preds = preds * dg.std.values + dg.mean.values\n",
    "    fcs = []\n",
    "    lev_idx = 0\n",
    "    for var, levels in OrderedDict({'z': None, 't': None}).items():\n",
    "        if levels is None:\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx],\n",
    "                dims=['time', 'lat', 'lon'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += 1\n",
    "        else:\n",
    "            nlevs = len(levels)\n",
    "            fcs.append(xr.DataArray(\n",
    "                preds[:, :, :, lev_idx:lev_idx+nlevs],\n",
    "                dims=['time', 'lat', 'lon', 'level'],\n",
    "                coords={'time': dg.valid_time, 'lat': dg.ds.lat, 'lon': dg.ds.lon, 'level': levels},\n",
    "                name=var\n",
    "            ))\n",
    "            lev_idx += nlevs\n",
    "    return xr.merge(fcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "z500_valid = load_test_data(f'{DATADIR}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{DATADIR}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = create_predictions(cnn, dg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_rmse = compute_weighted_rmse(fc, valid).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt, dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  ()\n",
       "Data variables:\n",
       "    z        float64 893.2\n",
       "    t        float64 4.118</pre><div class='xr-wrap' hidden><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-fe656332-e0c2-4712-962f-bb2857e86cf9' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-fe656332-e0c2-4712-962f-bb2857e86cf9' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-fd84f9b2-a6c9-4d08-9889-15d7a82bf7a3' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-fd84f9b2-a6c9-4d08-9889-15d7a82bf7a3' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-5226bbd4-9092-4072-88f5-439d4f8d0ab1' class='xr-section-summary-in' type='checkbox'  checked><label for='section-5226bbd4-9092-4072-88f5-439d4f8d0ab1' class='xr-section-summary' >Data variables: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>z</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>893.2</div><input id='attrs-2ca358b5-b7da-4302-afe8-6e3a9318c136' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-2ca358b5-b7da-4302-afe8-6e3a9318c136' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d9d7ec7d-189e-4a97-9b33-add9e9cbd54d' class='xr-var-data-in' type='checkbox'><label for='data-d9d7ec7d-189e-4a97-9b33-add9e9cbd54d' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(893.15328371)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>t</span></div><div class='xr-var-dims'>()</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>4.118</div><input id='attrs-bf089aa7-a6ce-4c1f-a86b-220fdf8a054a' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-bf089aa7-a6ce-4c1f-a86b-220fdf8a054a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-c7990b73-1e48-4ac6-a934-5dbc5f0c8d29' class='xr-var-data-in' type='checkbox'><label for='data-c7990b73-1e48-4ac6-a934-5dbc5f0c8d29' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array(4.11806495)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-62af8cfd-b429-4976-ad90-c3396cc7215b' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-62af8cfd-b429-4976-ad90-c3396cc7215b' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  ()\n",
       "Data variables:\n",
       "    z        float64 893.2\n",
       "    t        float64 4.118"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 21 steps, validate for 21 steps\n",
      "Epoch 1/100\n",
      "20/21 [===========================>..] - ETA: 3s - loss: 8.9510"
     ]
    }
   ],
   "source": [
    "# checking with no simple connection\n",
    "\n",
    "cnn_nores = build_resnet_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 2), l2 = 1e-5, skip = False, dr = 0.1)\n",
    "cnn_nores.compile(keras.optimizers.Adam(5e-5), 'mse')\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=5,\n",
    "                        verbose=1, \n",
    "                        mode='auto'\n",
    "                    )\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = 'val_loss',\n",
    "            patience=2,\n",
    "            factor=0.2,\n",
    "            verbose=1)\n",
    "\n",
    "cnn_nores.fit(dg_train, epochs=100, validation_data=dg_valid, \n",
    "          callbacks=[early_stopping_callback, reduce_lr_callback]\n",
    "         )\n",
    "\n",
    "fc_nores = create_predictions(cnn_nores, dg_test)\n",
    "cnn_rmse_nores = compute_weighted_rmse(fc_nores, valid).compute()\n",
    "\n",
    "cnn_rmse_nores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6 (test1)",
   "language": "python",
   "name": "python3_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
