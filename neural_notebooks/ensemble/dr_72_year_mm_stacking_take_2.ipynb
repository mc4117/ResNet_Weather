{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Train a CNN\n",
    "\n",
    "Add more data but actually get more error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Depending on your combination of package versions, this can raise a lot of TF warnings... \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from src.score import *\n",
    "from collections import OrderedDict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "sns.set_context('notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "DATADIR = '/rds/general/user/mc4117/home/WeatherBench/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create data generator\n",
    "\n",
    "First up, we want to write our own Keras data generator. The key advantage to just feeding in numpy arrays is that we don't have to load the data twice because our intputs and outputs are the same data just offset by the lead time. Since the dataset is quite large and we might run out of CPU RAM this is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "Loading data into RAM\n",
      "(32, 32, 64, 10)\n",
      "(32, 32, 64, 2)\n"
     ]
    }
   ],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True, \n",
    "                 mean=None, std=None, output_vars=None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        level_names = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for long_var, params in var_dict.items():\n",
    "            if long_var == 'constants': \n",
    "                for var in params:\n",
    "                    data.append(ds[var].expand_dims(\n",
    "                        {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "                    ))\n",
    "                    level_names.append(var)\n",
    "            else:\n",
    "                var, levels = params\n",
    "                try:\n",
    "                    data.append(ds[var].sel(level=levels))\n",
    "                    level_names += [f'{var}_{level}' for level in levels]\n",
    "                except ValueError:\n",
    "                    data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "                    level_names.append(var)\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.data['level_names'] = xr.DataArray(\n",
    "            level_names, dims=['level'], coords={'level': self.data.level})\n",
    "        if output_vars is None:\n",
    "            self.output_idxs = range(len(dg_valid.data.level))\n",
    "        else:\n",
    "            self.output_idxs = [i for i, l in enumerate(self.data.level_names.values) \n",
    "                                if any([bool(re.match(o, l)) for o in output_vars])]\n",
    "        \n",
    "        # Normalize\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "#         self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        self.std = self.data.std(('time', 'lat', 'lon')).compute() if std is None else std\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "        \n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time, level=self.output_idxs).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)\n",
    "\n",
    "var_dict = {\n",
    "    'geopotential': ('z', [500, 850]),\n",
    "    'temperature': ('t', [500, 850]),\n",
    "    'specific_humidity': ('q', [850]),\n",
    "    '2m_temperature': ('t2m', None),\n",
    "    'potential_vorticity': ('pv', [50, 100]),\n",
    "    'constants': ['lsm', 'orography']\n",
    "}\n",
    "\n",
    "ds = [xr.open_mfdataset(f'{DATADIR}/{var}/*.nc', combine='by_coords') for var in var_dict.keys()]\n",
    "\n",
    "ds_whole = xr.merge(ds)\n",
    "\n",
    "ds_train = ds_whole.sel(time=slice('2014', '2015'))\n",
    "ds_valid = ds_whole.sel(time=slice('2016', '2016'))\n",
    "ds_test = ds_whole.sel(time=slice('2017', '2018'))\n",
    "\n",
    "bs=32\n",
    "lead_time=72\n",
    "output_vars = ['z_500', 't_850']\n",
    "\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(ds_train, var_dict, lead_time, batch_size=bs, load=True, \n",
    "                         output_vars=output_vars)\n",
    "dg_valid = DataGenerator(ds_valid, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, \n",
    "                         shuffle=False, output_vars=output_vars)\n",
    "\n",
    "dg_test = DataGenerator(ds_test, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, \n",
    "                         shuffle=False, output_vars=output_vars)\n",
    "\n",
    "X, y = dg_train[0]; \n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeriodicPadding2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, pad_width, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pad_width = pad_width\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        if self.pad_width == 0:\n",
    "            return inputs\n",
    "        inputs_padded = tf.concat(\n",
    "            [inputs[:, :, -self.pad_width:, :], inputs, inputs[:, :, :self.pad_width, :]], axis=2)\n",
    "        # Zero padding in the lat direction\n",
    "        inputs_padded = tf.pad(inputs_padded, [[0, 0], [self.pad_width, self.pad_width], [0, 0], [0, 0]])\n",
    "        return inputs_padded\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'pad_width': self.pad_width})\n",
    "        return config\n",
    "\n",
    "\n",
    "class PeriodicConv2D(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters,\n",
    "                 kernel_size,\n",
    "                 conv_kwargs={},\n",
    "                 **kwargs, ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv_kwargs = conv_kwargs\n",
    "        if type(kernel_size) is not int:\n",
    "            assert kernel_size[0] == kernel_size[1], 'PeriodicConv2D only works for square kernels'\n",
    "            kernel_size = kernel_size[0]\n",
    "        pad_width = (kernel_size - 1) // 2\n",
    "        self.padding = PeriodicPadding2D(pad_width)\n",
    "        self.conv = Conv2D(\n",
    "            filters, kernel_size, padding='valid', **conv_kwargs\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.conv(self.padding(inputs))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'filters': self.filters, 'kernel_size': self.kernel_size, 'conv_kwargs': self.conv_kwargs})\n",
    "        return config\n",
    "    \n",
    "def build_cnn(filters, kernels, input_shape, dr=0):\n",
    "    \"\"\"Fully convolutional network\"\"\"\n",
    "    x = input = Input(shape=input_shape)\n",
    "    for f, k in zip(filters[:-1], kernels[:-1]):\n",
    "        x = PeriodicConv2D(f, k)(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        if dr > 0: x = Dropout(dr)(x, training = True)\n",
    "    output = PeriodicConv2D(filters[-1], kernels[-1])(x)\n",
    "    return keras.models.Model(input, output)\n",
    "\n",
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create non-iterative predictions\"\"\"\n",
    "    preds = xr.DataArray(\n",
    "        model.predict_generator(dg),\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg.valid_time, 'lat': dg.data.lat, 'lon': dg.data.lon, \n",
    "                'level': dg.data.isel(level=dg.output_idxs).level,\n",
    "                'level_names': dg.data.isel(level=dg.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "    # Unnormalize\n",
    "    preds = (preds * dg.std.isel(level=dg.output_idxs).values + \n",
    "             dg.mean.isel(level=dg.output_idxs).values)\n",
    "    unique_vars = list(set([l.split('_')[0] for l in preds.level_names.values])); unique_vars\n",
    "    \n",
    "    das = []\n",
    "    for v in unique_vars:\n",
    "        idxs = [i for i, vv in enumerate(preds.level_names.values) if vv.split('_')[0] in v]\n",
    "        #print(v, idxs)\n",
    "        da = preds.isel(level=idxs).squeeze().drop('level_names')\n",
    "        if not 'level' in da.dims: da.drop('level')\n",
    "        das.append({v: da})\n",
    "    return xr.merge(das, compat = 'override').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_1 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_1.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_1.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_0.h5')\n",
    "\n",
    "cnn_2 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_2.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_2.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_1.h5')\n",
    "\n",
    "cnn_3 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_3.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_3.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_2.h5')\n",
    "\n",
    "cnn_4 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_4.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_4.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_3.h5')\n",
    "\n",
    "cnn_5 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_5.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_5.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_4.h5')\n",
    "\n",
    "cnn_6 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_6.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_6.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_5.h5')\n",
    "\n",
    "cnn_7 = build_cnn([64, 64, 64, 64, 2], [5, 5, 5, 5, 5], (32, 64, 10), dr = 0.1)\n",
    "cnn_7.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "cnn_7.load_weights('/rds/general/user/mc4117/ephemeral/saved_models/train_72_multi_data_gpu_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [cnn_1, cnn_2, cnn_3, cnn_4, cnn_5, cnn_6, cnn_7]\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    model = model_list[i]\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        layer._name = 'ensemble_' + str(i+1) + '_' + layer.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# define multi-headed input\n",
    "ensemble_visible = [model.input for model in model_list]\n",
    "# concatenate merge output from each model\n",
    "ensemble_outputs = [model.output for model in model_list]\n",
    "merge = concatenate(ensemble_outputs)\n",
    "hidden = Dense(10, activation='relu')(merge)\n",
    "output = Dense(2)(hidden)\n",
    "ensemble_model = Model(inputs=ensemble_visible, outputs=output)\n",
    "# plot graph of ensemble\n",
    "plot_model(ensemble_model, show_shapes=True)\n",
    "# compile\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "ensemble_model.compile(keras.optimizers.Adam(1e-4), 'mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the model the training data again\n",
    "\n",
    "X1, y1 = dg_train[0]\n",
    "\n",
    "for i in range(1, len(dg_train)):\n",
    "    X2, y2 = dg_train[i]\n",
    "    X1 = np.concatenate((X1, X2))\n",
    "    y1 = np.concatenate((y1, y2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, 32, 64, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17448, 32, 64, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13958 samples, validate on 3490 samples\n",
      "Epoch 1/20\n",
      "13958/13958 [==============================] - 2013s 144ms/sample - loss: 1.6717 - val_loss: 0.7678\n",
      "Epoch 2/20\n",
      "13958/13958 [==============================] - 1448s 104ms/sample - loss: 0.3544 - val_loss: 0.1393\n",
      "Epoch 3/20\n",
      " 6816/13958 [=============>................] - ETA: 9:55 - loss: 0.1140"
     ]
    }
   ],
   "source": [
    "X = [X1 for _ in range(len(model_list))]\n",
    "# fit model\n",
    "ensemble_model.fit(X, y1, shuffle = True, epochs=20, validation_split = 0.2, verbose=1, callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=1,\n",
    "                        verbose=1, \n",
    "                        mode='auto'\n",
    "                    )])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions(model, dg):\n",
    "    \"\"\"Create non-iterative predictions\"\"\"\n",
    "    preds = xr.DataArray(\n",
    "        model.predict_generator(dg),\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg.valid_time, 'lat': dg.data.lat, 'lon': dg.data.lon, \n",
    "                'level': dg.data.isel(level=dg.output_idxs).level,\n",
    "                'level_names': dg.data.isel(level=dg.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "    # Unnormalize\n",
    "    preds = (preds * dg.std.isel(level=dg.output_idxs).values + \n",
    "             dg.mean.isel(level=dg.output_idxs).values)\n",
    "    unique_vars = list(set([l.split('_')[0] for l in preds.level_names.values])); unique_vars\n",
    "    \n",
    "    das = []\n",
    "    for v in unique_vars:\n",
    "        idxs = [i for i, vv in enumerate(preds.level_names.values) if vv.split('_')[0] in v]\n",
    "        print(v, idxs)\n",
    "        da = preds.isel(level=idxs).squeeze().drop('level_names')\n",
    "        if not 'level' in da.dims: da.drop('level')\n",
    "        das.append({v: da})\n",
    "    return xr.merge(das, compat = 'override').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model.save_weights('/rds/general/user/mc4117/ephemeral/saved_models/ensemble_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_1 = create_predictions(cnn_1, dg_test)\n",
    "fc_2 = create_predictions(cnn_2, dg_test)\n",
    "fc_3 = create_predictions(cnn_3, dg_test)\n",
    "fc_4 = create_predictions(cnn_4, dg_test)\n",
    "fc_5 = create_predictions(cnn_5, dg_test)\n",
    "fc_6 = create_predictions(cnn_6, dg_test)\n",
    "fc_7 = create_predictions(cnn_7, dg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the model the training data again\n",
    "\n",
    "X1_test, y1_test = dg_test[0]\n",
    "\n",
    "for i in range(1, len(dg_test)):\n",
    "    X2_test, y2_test = dg_test[i]\n",
    "    X1_test = np.concatenate((X1_test, X2_test))\n",
    "    y1_test = np.concatenate((y1_test, y2_test))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [X1_test for _ in range(len(model_list))]\n",
    "# make prediction\n",
    "fc_ensemble = ensemble_model.predict(X_test)\n",
    "\n",
    "#create_predictions(ensemble_model, X_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = xr.DataArray(\n",
    "        fc_ensemble,\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg_test.valid_time, 'lat': dg_test.data.lat, 'lon': dg_test.data.lon, \n",
    "                'level': dg_test.data.isel(level=dg_test.output_idxs).level,\n",
    "                'level_names': dg_test.data.isel(level=dg_test.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "# Unnormalize\n",
    "preds = (preds * dg_test.std.isel(level=dg_test.output_idxs).values + \n",
    "        dg_test.mean.isel(level=dg_test.output_idxs).values)\n",
    "unique_vars = list(set([l.split('_')[0] for l in preds.level_names.values])); unique_vars\n",
    "    \n",
    "das = []\n",
    "for v in unique_vars:\n",
    "    idxs = [i for i, vv in enumerate(preds.level_names.values) if vv.split('_')[0] in v]\n",
    "    print(v, idxs)\n",
    "    da = preds.isel(level=idxs).squeeze().drop('level_names')\n",
    "    if not 'level' in da.dims: da.drop('level')\n",
    "    das.append({v: da})\n",
    "fc_ensemble_unnorm = xr.merge(das, compat = 'override').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_ensemble_unnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_unnorm =y1_test* dg_test.std.isel(level=[0,3]).values+dg_test.mean.isel(level=[0,3]).values\n",
    "\n",
    "real_ds = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        real_unnorm[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        real_unnorm[..., 1],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse(fc_ensemble_unnorm, real_ds).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse(fc_1, real_ds).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_ensemble_train = ensemble_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = xr.DataArray(\n",
    "        fc_ensemble_train,\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg_train.valid_time, 'lat': dg_train.data.lat, 'lon': dg_train.data.lon, \n",
    "                'level': dg_train.data.isel(level=dg_train.output_idxs).level,\n",
    "                'level_names': dg_train.data.isel(level=dg_train.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "# Unnormalize\n",
    "preds = (preds * dg_train.std.isel(level=dg_train.output_idxs).values + \n",
    "        dg_train.mean.isel(level=dg_train.output_idxs).values)\n",
    "unique_vars = list(set([l.split('_')[0] for l in preds.level_names.values])); unique_vars\n",
    "    \n",
    "das = []\n",
    "for v in unique_vars:\n",
    "    idxs = [i for i, vv in enumerate(preds.level_names.values) if vv.split('_')[0] in v]\n",
    "    print(v, idxs)\n",
    "    da = preds.isel(level=idxs).squeeze().drop('level_names')\n",
    "    if not 'level' in da.dims: da.drop('level')\n",
    "    das.append({v: da})\n",
    "fc_ensemble_train_unnorm = xr.merge(das, compat = 'override').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_unnorm_train =y1* dg_train.std.isel(level=[0,3]).values+dg_train.mean.isel(level=[0,3]).values\n",
    "\n",
    "real_ds_train = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        real_unnorm_train[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_train.data.time[72:], 'lat': dg_train.data.lat, 'lon': dg_train.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        real_unnorm_train[..., 1],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_train.data.time[72:], 'lat': dg_train.data.lat, 'lon': dg_train.data.lon,\n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "compute_weighted_rmse(fc_ensemble_train_unnorm, real_ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6 (test1)",
   "language": "python",
   "name": "python3_test1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
