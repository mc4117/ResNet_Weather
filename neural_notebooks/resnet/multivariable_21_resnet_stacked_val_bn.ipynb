{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import *\n",
    "import tensorflow.keras.backend as K\n",
    "from src.score import *\n",
    "import re\n",
    "\n",
    "import generate_data as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet/whole_mm_indiv_new_val.py using 9 blocks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "if var_name == 'specific_humidity':\n",
    "    if args.level_list is None:\n",
    "        unique_list = [300, 500, 600, 700, 850, 925, 1000]\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        'specific_humidity': ('q', unique_list)}\n",
    "elif var_name == '2m temp':\n",
    "    unique_list = None\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        '2m_temperature': ('t2m', None)}\n",
    "elif var_name == 'pot_vort':\n",
    "    if args.level_list is None:\n",
    "        unique_list = [150, 250, 300, 700, 850]  \n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        'potential_vorticity': ('pv', unique_list)}\n",
    "elif var_name == 'const':\n",
    "    unique_list\t= None\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850]),\n",
    "        'constants': ['lat2d', 'orography', 'lsm']}\n",
    "elif var_name == 'orig':\n",
    "    unique_list\t= None\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', [850])} \n",
    "elif var_name == 'temp':\n",
    "    if args.level_list is None:\n",
    "        unique_list = [300, 400, 500, 600, 700, 850]\n",
    "    else:\n",
    "        unique_list.append(850)\n",
    "        unique_list = sorted(list(dict.fromkeys(unique_list)))\n",
    "    print(unique_list)\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', [500]),\n",
    "        'temperature': ('t', unique_list)}\n",
    "elif var_name == 'geo':\n",
    "    if args.level_list is None:\n",
    "        unique_list = [300, 400, 500, 600, 700, 850]\n",
    "    else:\n",
    "        unique_list.append(500)\n",
    "        unique_list = sorted(list(dict.fromkeys(unique_list)))\n",
    "    var_dict = {\n",
    "        'geopotential': ('z', unique_list),\n",
    "        'temperature': ('t', [850])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = '/rds/general/user/mc4117/home/WeatherBench/data/'\n",
    "\n",
    "z500_valid = load_test_data(f'{DATADIR}geopotential_500', 'z')\n",
    "t850_valid = load_test_data(f'{DATADIR}temperature_850', 't')\n",
    "valid = xr.merge([z500_valid, t850_valid])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "var_dict = {\n",
    "    'geopotential': ('z', [500]),\n",
    "    'temperature': ('t', [850]),\n",
    "}\n",
    "\n",
    "# For the data generator all variables have to be merged into a single dataset.\n",
    "ds = [xr.open_mfdataset(f'{DATADIR}/{var}/*.nc', combine='by_coords') for var in var_dict.keys()]\n",
    "ds_whole = xr.merge(ds, compat = 'override')\n",
    "\n",
    "# load all training data\n",
    "ds_train = ds_whole.sel(time=slice('1979', '2016'))\n",
    "ds_test = ds_whole.sel(time=slice('2017', '2018'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, ds, var_dict, lead_time, batch_size=32, shuffle=True, load=True,\n",
    "                 mean=None, std=None, output_vars=None):\n",
    "        \"\"\"\n",
    "        Data generator for WeatherBench data.\n",
    "        Template from https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "        Args:\n",
    "            ds: Dataset containing all variables\n",
    "            var_dict: Dictionary of the form {'var': level}. Use None for level if data is of single level\n",
    "            lead_time: Lead time in hours\n",
    "            batch_size: Batch size\n",
    "            shuffle: bool. If True, data is shuffled.\n",
    "            load: bool. If True, datadet is loaded into RAM.\n",
    "            mean: If None, compute mean from data.\n",
    "            std: If None, compute standard deviation from data.\n",
    "        \"\"\"\n",
    "\n",
    "        self.ds = ds\n",
    "        self.var_dict = var_dict\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lead_time = lead_time\n",
    "\n",
    "        data = []\n",
    "        level_names = []\n",
    "        generic_level = xr.DataArray([1], coords={'level': [1]}, dims=['level'])\n",
    "        for long_var, params in var_dict.items():\n",
    "            if long_var == 'constants':\n",
    "                for var in params:\n",
    "                    data.append(ds[var].expand_dims(\n",
    "                        {'level': generic_level, 'time': ds.time}, (1, 0)\n",
    "                    ))\n",
    "                    level_names.append(var)\n",
    "            else:\n",
    "                var, levels = params\n",
    "                try:\n",
    "                    data.append(ds[var].sel(level=levels))\n",
    "                    level_names += [f'{var}_{level}' for level in levels]\n",
    "                except ValueError:\n",
    "                    data.append(ds[var].expand_dims({'level': generic_level}, 1))\n",
    "                    level_names.append(var)\n",
    "\n",
    "        self.data = xr.concat(data, 'level').transpose('time', 'lat', 'lon', 'level')\n",
    "        self.data['level_names'] = xr.DataArray(\n",
    "            level_names, dims=['level'], coords={'level': self.data.level})\n",
    "        if output_vars is None:\n",
    "            self.output_idxs = range(len(dg_valid.data.level))\n",
    "        else:\n",
    "            self.output_idxs = [i for i, l in enumerate(self.data.level_names.values)\n",
    "                                if any([bool(re.match(o, l)) for o in output_vars])]\n",
    "\n",
    "        # Normalize\n",
    "        self.mean = self.data.mean(('time', 'lat', 'lon')).compute() if mean is None else mean\n",
    "#         self.std = self.data.std('time').mean(('lat', 'lon')).compute() if std is None else std\n",
    "        self.std = self.data.std(('time', 'lat', 'lon')).compute() if std is None else std\n",
    "        self.data = (self.data - self.mean) / self.std\n",
    "\n",
    "        self.n_samples = self.data.isel(time=slice(0, -lead_time)).shape[0]\n",
    "        self.init_time = self.data.isel(time=slice(None, -lead_time)).time\n",
    "        self.valid_time = self.data.isel(time=slice(lead_time, None)).time\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "        # For some weird reason calling .load() earlier messes up the mean and std computations\n",
    "        #if load: print('Loading data into RAM'); self.data.load()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.ceil(self.n_samples / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        'Generate one batch of data'\n",
    "        idxs = self.idxs[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "        X = self.data.isel(time=idxs).values\n",
    "        y = self.data.isel(time=idxs + self.lead_time, level=self.output_idxs).values\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.idxs = np.arange(self.n_samples)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.idxs)        \n",
    "            \n",
    "bs=32\n",
    "lead_time=72\n",
    "output_vars = ['z_500', 't_850']\n",
    "\n",
    "# Create a training and validation data generator. Use the train mean and std for validation as well.\n",
    "dg_train = DataGenerator(\n",
    "    ds_train.sel(time=slice('1979', '2013')), var_dict, lead_time, batch_size=bs, load=True, output_vars = output_vars)\n",
    "\n",
    "#dg_valid2 = DataGenerator(\n",
    "#    ds_train.sel(time=slice('2015', '2016')), var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False, output_vars = output_vars)\n",
    "\n",
    "dg_valid = DataGenerator(\n",
    "    ds_train.sel(time=slice('2015', '2016')), var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std, shuffle=False, output_vars = output_vars)\n",
    "\n",
    "# Now also a generator for testing. Impartant: Shuffle must be False!\n",
    "dg_test = DataGenerator(ds_test, var_dict, lead_time, batch_size=bs, mean=dg_train.mean, std=dg_train.std,\n",
    "                         shuffle=False, output_vars=output_vars)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1 = dg_valid[0]\n",
    "\n",
    "for i in range(1, len(dg_valid)):\n",
    "    X2, y2 = dg_valid[i]\n",
    "    X1 = np.concatenate((X1, X2))\n",
    "    y1 = np.concatenate((y1, y2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_unnorm =y1* dg_valid.std.isel(level=dg_valid.output_idxs).values+dg_test.mean.isel(level=dg_valid.output_idxs).values\n",
    "\n",
    "real_ds = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        real_unnorm[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid.data.time[72:], 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        real_unnorm[..., 1],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid.data.time[72:], 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                },\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in outputs\n",
    "\n",
    "temp_levels = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/21_temp_[300, 400, 500, 600, 700, 850]_preds_newval.nc')\n",
    "geo_levels = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/21_geo_[300, 400, 500, 600, 700, 850]_preds_newval.nc')\n",
    "\n",
    "#sh = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/9_specific_humidity_[300, 500, 600, 700, 850, 925, 1000]_preds_newval.nc')\n",
    "#pv = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/9_pot_vort_[150, 250, 300, 700, 850]_preds_newval.nc')\n",
    "const = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/21_const_None_preds_newval.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rearranged = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        pv.z.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid.data.time[72:], 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        pv.t.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid.data.time[72:], 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "temp_2m_rearranged = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        temp_2m.z.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid.data.time[72:], 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        temp_2m.t.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid.data.time[72:], 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "const_rearranged = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        const.z.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid.data.time[72:], 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        const.t.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_valid.data.time[72:], 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                },\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_z_t = dg_test.mean.isel(level=dg_test.output_idxs).values\n",
    "std_z_t = dg_test.std.isel(level=dg_test.output_idxs).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 645\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        float64 367.3\n",
      "    t        float64 2.087\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    z        float64 369.0\n",
      "    t        float64 2.079\n",
      "<xarray.Dataset>\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    t        float64 2.213\n",
      "    z        float64 421.4\n"
     ]
    }
   ],
   "source": [
    "print(compute_weighted_rmse(temp_levels, real_ds))\n",
    "print(compute_weighted_rmse(geo_levels, real_ds))\n",
    "#print(compute_weighted_rmse(sh, real_ds))\n",
    "#print(compute_weighted_rmse(pv_rearranged, real_ds))\n",
    "print(compute_weighted_rmse(const, real_ds))\n",
    "#print(compute_weighted_rmse(temp_2m, real_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_norm = (np.transpose(temp_levels_rearranged.to_array().data, axes = [1, 2, 3, 0]) - mean_z_t)/std_z_t\n",
    "geo_norm = (np.transpose(geo_levels.to_array().data, axes = [1, 2, 3, 0]) - mean_z_t)/std_z_t\n",
    "\n",
    "sh_norm = (np.transpose(sh.to_array().data, axes = [1, 2, 3, 0])-mean_z_t)/std_z_t\n",
    "pv_norm = (np.transpose(pv_rearranged.to_array().data, axes = [1, 2, 3, 0])-mean_z_t)/std_z_t\n",
    "const_norm = (np.transpose(const.to_array().data, axes = [1, 2, 3, 0])-mean_z_t)/std_z_t\n",
    "temp_2m_norm = (np.transpose(temp_2m_rearranged.to_array().data, axes = [1, 2, 3, 0])-mean_z_t)/std_z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_test_list = [temp_norm, geo_norm, temp_2m_norm, sh_norm, pv_norm, const_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "def my_init(shape, dtype=None):\n",
    "    print(shape)\n",
    "    return tf.ones(shape, dtype=dtype)/6\n",
    "\n",
    "def build_stack_model(input_shape, stack_list):\n",
    "    # concatenate merge output from each model\n",
    "    input_list = [Input(shape=input_shape) for i in range(len(stack_list))]\n",
    "    merge = concatenate(input_list)\n",
    "    hidden = Dense(25, activation='relu', kernel_initializer = my_init)(merge)\n",
    "    normalize2 = BatchNormalization()(hidden)\n",
    "    output = Dense(2)(normalize2)\n",
    "    return keras.models.Model(input_list, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = build_stack_model((32, 64, 2), stack_test_list)\n",
    "\n",
    "ensemble_model.compile(keras.optimizers.Adam(1e-4), 'mse')\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "                        monitor='val_loss',\n",
    "                        min_delta=0,\n",
    "                        patience=10,\n",
    "                        verbose=1, \n",
    "                        mode='auto'\n",
    "                    )\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = 'val_loss',\n",
    "            patience=3,\n",
    "            factor=0.2,\n",
    "            verbose=1)  \n",
    "\n",
    "ensemble_model.fit(x = stack_test_list, y = y1, epochs = 600, validation_split = 0.2, shuffle = True\n",
    "                  , callbacks = [early_stopping_callback, reduce_lr_callback\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble_model.save_weights('stacked2.h5')\n",
    "ensemble_model.save_weights('stacked_val_comb_9_with2m.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = ensemble_model.predict(stack_test_list)\n",
    "preds_un = xr.DataArray(\n",
    "        fc,\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg_valid.valid_time, 'lat': dg_valid.data.lat, 'lon': dg_valid.data.lon,\n",
    "                'level': dg_valid.data.isel(level=dg_valid.output_idxs).level,\n",
    "                'level_names': dg_valid.data.isel(level=dg_valid.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "# Unnormalize\n",
    "preds = preds_un * std_z_t + mean_z_t\n",
    "unique_vars = list(set([l.split('_')[0] for l in preds.level_names.values])); unique_vars\n",
    "\n",
    "das = []\n",
    "for v in unique_vars:\n",
    "        idxs = [i for i, vv in enumerate(preds.level_names.values) if vv.split('_')[0] in v]\n",
    "        #print(v, idxs)\n",
    "        da = preds.isel(level=idxs).squeeze().drop('level_names')\n",
    "        if not 'level' in da.dims: da.drop('level')\n",
    "        das.append({v: da})\n",
    "fc_unnorm = xr.merge(das, compat = 'override').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_unnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse(fc_unnorm, real_ds).compute() # got 385.6 with 2m temp so very slight improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse((geo_levels + temp_levels + sh + const + pv + temp_2m)/6, real_ds) # so improvement on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, y1_test = dg_test[0]\n",
    "\n",
    "for i in range(1, len(dg_test)):\n",
    "    X2, y2 = dg_test[i]\n",
    "    X1 = np.concatenate((X1, X2))\n",
    "    y1_test = np.concatenate((y1_test, y2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_unnorm_test =y1_test* dg_test.std.isel(level=dg_test.output_idxs).values+dg_test.mean.isel(level=dg_test.output_idxs).values\n",
    "\n",
    "real_ds_test = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        real_unnorm_test[..., 0],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        real_unnorm_test[..., 1],\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in outputs\n",
    "\n",
    "temp_levels_test = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/9_temp_[300, 400, 500, 600, 700, 850]_preds_newtest.nc')\n",
    "geo_levels_test = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/9_geo_[300, 400, 500, 600, 700, 850]_preds_newtest.nc')\n",
    "\n",
    "sh_test = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/9_specific_humidity_[300, 500, 600, 700, 850, 925, 1000]_preds_newtest.nc')\n",
    "pv_test = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/9_pot_vort_[150, 250, 300, 700, 850]_preds_newtest.nc')\n",
    "const_test = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/9_const_None_preds_newtest.nc')\n",
    "temp_2m_test = xr.open_dataset('/rds/general/user/mc4117/home/WeatherBench/saved_pred_data/9_const_None_preds_newtest.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2m_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_rearranged_test = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        pv_test.z.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        pv_test.t.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "temp_levels_rearranged_test = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        temp_levels_test.z.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        temp_levels_test.t.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    )\n",
    "})\n",
    "\n",
    "geo_levels_rearranged_test = xr.Dataset({\n",
    "    'z': xr.DataArray(\n",
    "        geo_levels_test.z.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    ),\n",
    "    't': xr.DataArray(\n",
    "        geo_levels_test.t.values,\n",
    "        dims=['time', 'lat', 'lon'],\n",
    "        coords={'time':dg_test.data.time[72:], 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                },\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_norm = (np.transpose(temp_levels_rearranged_test.to_array().data, axes = [1, 2, 3, 0]) - mean_z_t)/std_z_t\n",
    "geo_test_norm = (np.transpose(geo_levels_rearranged_test.to_array().data, axes = [1, 2, 3, 0])- mean_z_t)/std_z_t\n",
    "\n",
    "sh_test_norm = (np.transpose(sh_test.to_array().data, axes = [1, 2, 3, 0])-mean_z_t)/std_z_t\n",
    "pv_test_norm = (np.transpose(pv_rearranged_test.to_array().data, axes = [1, 2, 3, 0])-mean_z_t)/std_z_t\n",
    "const_test_norm = (np.transpose(const_test.to_array().data, axes = [1, 2, 3, 0])-mean_z_t)/std_z_t\n",
    "test_2m_norm = (np.transpose(temp_2m_test.to_array().data, axes = [1, 2, 3, 0])-mean_z_t)/std_z_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_test_data_list = [temp_test_norm, geo_test_norm, temp_2m_norm, sh_test_norm, pv_test_norm, const_test_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    fc_test = ensemble_model.predict(stack_test_data_list)\n",
    "    preds_un = xr.DataArray(\n",
    "        fc_test,\n",
    "        dims=['time', 'lat', 'lon', 'level'],\n",
    "        coords={'time': dg_test.valid_time, 'lat': dg_test.data.lat, 'lon': dg_test.data.lon,\n",
    "                'level': dg_test.data.isel(level=dg_test.output_idxs).level,\n",
    "                'level_names': dg_test.data.isel(level=dg_test.output_idxs).level_names\n",
    "               },\n",
    "    )\n",
    "    # Unnormalize\n",
    "    preds = preds_un * std_z_t + mean_z_t\n",
    "    unique_vars = list(set([l.split('_')[0] for l in preds.level_names.values])); unique_vars\n",
    "\n",
    "    das = []\n",
    "    for v in unique_vars:\n",
    "        idxs = [i for i, vv in enumerate(preds.level_names.values) if vv.split('_')[0] in v]\n",
    "        #print(v, idxs)\n",
    "        da = preds.isel(level=idxs).squeeze().drop('level_names')\n",
    "        if not 'level' in da.dims: da.drop('level')\n",
    "        das.append({v: da})\n",
    "    fc_unnorm_test = xr.merge(das, compat = 'override').drop('level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse(fc_unnorm_test, real_ds_test).compute() # makes no difference if include 2m temp (see output files for job_nn_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse((temp_levels_test + geo_levels_test + temp_2m_test + sh_test + const_test + pv_test)/5, real_ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual \n",
    "print(compute_weighted_rmse(temp_levels_test, real_ds_test))\n",
    "print(compute_weighted_rmse(geo_levels_test, real_ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_weighted_rmse((temp_levels_test + geo_levels_test)/2, real_ds_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
